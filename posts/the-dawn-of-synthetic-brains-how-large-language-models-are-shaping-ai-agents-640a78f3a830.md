---
title: "The Dawn of Synthetic Brains: How Large Language Models Are Shaping AI Agents"
date: "2023-12-29"
coverImage: "https://cdn-images-1.medium.com/max/1024/1*T6XlrGGcdwnZTrhPRs3ENA.png"
canonical: "https://medium.com/@prdeepak.babu/the-dawn-of-synthetic-brains-how-large-language-models-are-shaping-ai-agents-640a78f3a830"
mediumId: "640a78f3a830"
source: "medium"
---

<section name="2130" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="dfe4" id="dfe4" class="graf graf--p graf-after--h3">Recent advancements in Large Language Models (LLMs) have demonstrated a significant capability for reasoning and planning, a trait predominantly seen in models with over 100 billion parameters (considered an <a href="https://medium.com/@prdeepak.babu/emergent-abilities-in-llm-unpredictable-abilities-in-large-language-models-72fc54059b41?source=your_stories_page-------------------------------------" data-href="https://medium.com/@prdeepak.babu/emergent-abilities-in-llm-unpredictable-abilities-in-large-language-models-72fc54059b41?source=your_stories_page-------------------------------------" class="markup--anchor markup--p-anchor" target="_blank">emergent ability</a>). While opinions differ on whether LLMs genuinely possess intrinsic reasoning abilities, it’s becoming increasingly evident that, through sophisticated prompting strategies, LLMs can be equipped with the necessary ‘tools’ to collaboratively tackle complex problems in a manner akin to human problem-solving. This has reignited interest in the development of Agents, a field that had largely remained in the realm of academic research for decades. In this article, we delve into the role of LLMs in advancing agent technology, examine their inherent limitations, and explore how current agent research is striving to overcome these challenges.</p><p name="a6a6" id="a6a6" class="graf graf--p graf-after--p">What exactly defines an agent? In simple terms, an agent is an entity capable of perceiving its environment and acting to alter that environment’s state. A crucial aspect of an agent is autonomy, or the ability to act independently without human supervision. The concepts of ‘state’ and ‘environment’ are central to this definition, and we’ll explore these shortly. Agents have their foundation in Reinforcement Learning, a branch of AI/ML focused on machines that learn from their own experiences. These experiences might be derived from self-play, as seen in AlphaGo’s mastery of Go through repeated self-play, or from imitating human interactions with the world. Take, for instance, a home security system: if a locked house represents a certain state, then the system’s action of unlocking the door represents a change in that state, with the home being the environment. It’s important to note that the concept of agents extends beyond the physical realm to include software agents. These digital agents can perform tasks like ordering a ride on Uber with a simple command, such as ‘<em class="markup--em markup--p-em">order Uber to Seatac airport for 5 people</em>,’ automating actions that would otherwise require manual input.</p><p name="df90" id="df90" class="graf graf--p graf-after--figure">The classification of agents can be understood along two key dimensions: (i) the breadth of tasks they can handle and (ii) the accuracy or performance level in executing these tasks. Historically, most agents have been specialized, adept at narrowly defined tasks, such as playing chess or Go. For instance, AlphaGo demonstrated super-human performance in this realm, famously defeating world champion Lee Sedol in March 2016 with a score of four games to one. On the other hand, voice assistants like Alexa or Siri represent a broader scope, performing tasks ranging from home automation, like turning lights on or off, to providing weather updates or navigating screens. While their performance can be considered on par with human capabilities, they often come across as somewhat templated in their responses. To realize the goal of Artificial General Intelligence (AGI), we require systems that not only cover a wide array of tasks but also match or exceed human performance levels in these tasks.</p><p name="84a5" id="84a5" class="graf graf--p graf-after--p">Drawing inspiration from human behavior, the critical components of an intelligent agent can be conceptualized as follows:</p><ol class="postList"><li name="3328" id="3328" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Brain</strong>: This is the central processing unit of the agent, tasked with interpreting environmental stimuli received through the perception module. It is responsible for decision-making, reasoning, and planning actions. Key functions include logical reasoning, forming analogies, and engaging in second-order thinking. The brain is the core of the agent, seamlessly integrating with perception, memory, and action modules.</li><li name="9d0d" id="9d0d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Perception</strong>: This module is akin to the human sensory system, responsible for sensing the environment. It includes vision (similar to eyes), speech and language processing (akin to ears), and can extend to other senses such as touch (comparable to skin) and olfaction (like the nose). Perception is crucial for providing the brain with necessary environmental information.</li><li name="809c" id="809c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Memory</strong>: Serving as the agent’s repository of experiences and facts, memory is essential for future retrieval of information. Often regarded as an integral part of the brain, it encompasses various aspects like habitual memory (for routine tasks like driving), short-term memory (for recent events), and long-term memory (for more distant recollections, such as childhood memories).</li><li name="3ffe" id="3ffe" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Action (Actuator)</strong>: This component can be likened to human actuators, such as hands and legs, which interact with and modify the environment. It can involve physical movements or communication methods like speech to convey thoughts and information.</li></ol><p name="6d14" id="6d14" class="graf graf--p graf-after--li">So how does LLM powered agents work ? what parts of this agent component is left unadressed or needs work around ?</p><figure name="46d4" id="46d4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*l1weTUNmJQT7gvI-R0JrXQ.png" src="https://cdn-images-1.medium.com/max/800/1*l1weTUNmJQT7gvI-R0JrXQ.png"><figcaption class="imageCaption">Components of Agent Architecture from the <a href="https://arxiv.org/pdf/2309.07864.pdf" data-href="https://arxiv.org/pdf/2309.07864.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">agents survey paper</a> by Fudan NLP group showing brain, perception and action modules.</figcaption></figure><h3 name="6d25" id="6d25" class="graf graf--h3 graf-after--figure">LLM as the brain</h3><p name="b01b" id="b01b" class="graf graf--p graf-after--h3">LLMs pretrained from large internet corpus text, audio and images can be considered as world models that can reason and plan much like the human brain and forms central aspect of agent architecture. Instruction fine-tuning makes the needed alignment with human-human like interaction, extending as an efficient interfaces for human-machine (HCI) interaction. Synthetically generated data has become a force multiplier in this effort. I recently wrote an in-depth blog post on this topic <a href="https://medium.com/@prdeepak.babu/harnessing-the-power-of-synthetic-data-in-the-era-of-large-language-models-llms-and-generative-ai-f67763256d44?source=your_stories_page-------------------------------------" data-href="https://medium.com/@prdeepak.babu/harnessing-the-power-of-synthetic-data-in-the-era-of-large-language-models-llms-and-generative-ai-f67763256d44?source=your_stories_page-------------------------------------" class="markup--anchor markup--p-anchor" target="_blank">here</a>. Prompting has become the way to elicit desired response from LLMs and leading to an emerging field of Prompt Engineering. While LLMs auto-regressive nature of generation has been shown to be a bottleneck in solving complex problems that require multi-step reasoning. We have worked around this limitation, by using advanced prompt engineering techniques like ReACT (Refine Act), Refine, Reflexion, CoT(Chain-of-thought), ToT (Tree-of-thought), etc. The fundamental idea behind these techniques is to ask the LLM to breakdown the solution into steps and template by using the LLM to fill-in the template to form a coherent solution to a complex problem. While there are overlaps in tecniques with minor modification, we can consider two class of prompting (i) sequential problem solving — problem is broken down as sequential steps (LLM conditioned gen. on previous step) and (ii) plan ahead solving that parallelizes problem into sub-problems and independently executes sub-problem speeding up the inference. BabyAGI and AutoGPT are two example frameworks that make use of these strategies to solve complex decision making and reasoning problems. There is another class of techniques that focus on automated prompt engineering (APE) to identify the best way to prompt a LLM for a desired task.</p><h3 name="d254" id="d254" class="graf graf--h3 graf-after--p">Images, Text, and Audio: Multimodal Perception in LLMs</h3><p name="3d56" id="3d56" class="graf graf--p graf-after--h3">The evolution of Large Language Models (LLMs) has progressed from solely focusing on extensive text corpora to integrating multimodal learning that encompasses text, images, and audio. This advancement is exemplified by models like Google DeepMind’s Gemini, which processes these modalities in unison. Multimodality is essential for creating an accurate representation of the state, crucial for effective reasoning. For example, in activity detection with a smartwatch, relying only on wrist movement data could be misleading. Incorporating additional sensory inputs like acceleration and GPS readings can more accurately confirm activities like walking. I had recently covered this in some depth as blog post <a href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f?source=your_stories_page-------------------------------------" data-href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f?source=your_stories_page-------------------------------------" class="markup--anchor markup--p-anchor" target="_blank">here</a>.</p><p name="e4b3" id="e4b3" class="graf graf--p graf-after--p">At their core, Multimodal LLMs convert each modality into a dense representation, either through pretrained embeddings or on-the-fly learning. This allows for the conditional generation of outputs in various modalities in an interleaved manner. These LLMs typically employ self-supervised learning approaches, such as those used in BERT, to predict masked tokens from mixed modalities. Cross-modal alignment is facilitated through supervised learning, involving joint modality prediction tasks. For instance, CLIP (Contrastive Language Image Pretraining) utilizes an image captioning task to align image and text modalities. Similarly, CLAP (Contrastive Language Audio Pretraining) aligns audio and text by employing audio classification tasks.</p><p name="a20d" id="a20d" class="graf graf--p graf-after--p">Unlike text, which is tokenized into discrete units like characters or sub-words, audio and image data are continuous signals. Therefore, they require discretization and the development of a unified codebook to be effectively integrated into the LLM framework.</p><figure name="8c3b" id="8c3b" class="graf graf--figure graf--iframe graf-after--p"></figure><pre data-code-block-mode="0" spellcheck="false" name="c3fc" id="c3fc" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content">great lecture on mutimodal LLMs from stanford CS244n</span></pre><h3 name="f76e" id="f76e" class="graf graf--h3 graf-after--pre">Memory in LLMs: Context Windows and Vector Databases</h3><p name="6de5" id="6de5" class="graf graf--p graf-after--h3">In Large Language Models (LLMs), the context window — utilized for providing instructions, examples, and contextual cues — acts as a form of short-term memory. This memory spans a few dialogue turns but is relatively limited in size, ranging from 2K to 100K tokens in state-of-the-art models. For long-term memory and fact retrieval, vector databases like Pinecone have been employed, leveraging dense embedding retrieval methods. Additionally, the parametric memory within LLMs’ weights, acquired during pretraining, represents a form of knowledge storage. However, this knowledge can be less reliable, sometimes leading to hallucinated facts and outputs, due to the varying quality of pretraining data.</p><p name="b78d" id="b78d" class="graf graf--p graf-after--p">Memory in LLMs is a complex topic. Retrieving relevant information from extensive knowledge bases or past transactions poses significant challenges, especially when multiple historical events may be pertinent to a given query. For instance, the ‘<a href="https://arxiv.org/abs/2304.03442" data-href="https://arxiv.org/abs/2304.03442" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Generative Agents</a>’ paper from Stanford proposes a retrieval method based on a weighted combination of relevance and recency for simulated agent societies. Meanwhile, the ‘<a href="https://arxiv.org/abs/2309.02427" data-href="https://arxiv.org/abs/2309.02427" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Cognitive Architecture of Language Agents</a>’ paper by Summers and Yao highlights the necessity for different memory types — long-term procedural memory, semantic memory for facts, and episodic memory for past interactions. There are ongoing efforts to understand human brain and memory functioning and to apply or augment these insights to agent architecture, developing efficient data structures for storing facts and events.</p><p name="968b" id="968b" class="graf graf--p graf-after--p">A critical issue in this domain is determining the appropriate size and boundaries for information chunks. Overly large chunks may exceed LLMs’ context length limits, while incorrectly defined or too small chunks can result in information loss or the need for extensive processing. When faced with multiple relevant passages for a query, a common approach involves summarizing or applying a secondary level of filtering as a preliminary step to fit within the LLMs’ context window.</p><p name="0d72" id="0d72" class="graf graf--p graf-after--p">For example, consider the question, “<em class="markup--em markup--p-em">Have I ever visited this place</em>?” This inquiry involves several layers: (i) identifying the current location (‘<em class="markup--em markup--p-em">this</em>’), (ii) determining the subject (‘<em class="markup--em markup--p-em">I</em>’), and (iii) searching through spatial information for past visits (‘<em class="markup--em markup--p-em">ever visited</em>’). Simple storage of experiences as text blobs is inadequate; a more structured approach, potentially involving a taxonomy or hierarchy, is necessary for effective information organization and retrieval.</p><h3 name="6d6d" id="6d6d" class="graf graf--h3 graf-after--p">Robotic Arms and APIs as actuators</h3><p name="d8d1" id="d8d1" class="graf graf--p graf-after--h3">In the realm of software agents, actuators can take the form of APIs and tools. For instance, a calculator API may be used for mathematical tasks, and knowledge graph queries can address factual questions. Additionally, software agents may simulate interactions like clicking, typing, and touch for seamless web and app navigation. In contrast, embodied agents, such as robots equipped with arms and legs, have physical counterparts to these APIs and tools, functioning as their actuators.</p><p name="9f9d" id="9f9d" class="graf graf--p graf-after--p">Utilizing tools and APIs is particularly crucial in addressing hallucination issues common in Large Language Models (LLMs). This is especially relevant for tasks like complex calculations, where, akin to human reliance on calculators, LLMs can benefit from API assistance. An example of this is <a href="https://arxiv.org/abs/2303.17580" data-href="https://arxiv.org/abs/2303.17580" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">HuggingGPT’s</a> approach of providing LLMs access to every model in the Hugging Face hub as APIs for multi-step, higher-order tasks. Imagine a scenario where an LLM can access and utilize a full suite of AWS or Azure services as tools and APIs. This integration would enable the LLM to parse inputs and outputs for each tool, orchestrating them to accomplish complex tasks. <a href="https://arxiv.org/pdf/2302.04761.pdf" data-href="https://arxiv.org/pdf/2302.04761.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Toolformer</a> paper discusses teaching LLMs to use tools. More recent works involve creation of tools on the fly and using them to solve complex problems.</p><p name="0b00" id="0b00" class="graf graf--p graf-after--p">For instance, in digitizing paper bills or receipts, an LLM could invoke an OCR (Optical Character Recognition) tool for image-to-text conversion, followed by entity extraction for items and prices, and subsequently format the data into a desired layout, like a table, before logging it into a spreadsheet.</p><p name="516a" id="516a" class="graf graf--p graf-after--p"><a href="https://python.langchain.com/docs/modules/agents/" data-href="https://python.langchain.com/docs/modules/agents/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LangChain</a> is an innovative framework that provides developers and scientists with the necessary abstractions to build agents. It integrates various LLMs, memory databases, and both first-party (1P) and third-party (3P) APIs, along with essential tools like file readers and writers. LangChain also incorporates popular reasoning frameworks, such as ReACT, AutoGPT, and BabyAGI, facilitating the easy integration of different LLMs and APIs for diverse applications.</p><h3 name="97b9" id="97b9" class="graf graf--h3 graf-after--p">Concluding Thoughts</h3><p name="35b9" id="35b9" class="graf graf--p graf-after--h3">While Large Language Models (LLMs) have significantly advanced the prospects of Artificial General Intelligence (AGI) agents, key challenges remain, particularly in learning efficiency (or sample efficiency) and long-term memory management. The current training methods for LLMs, which involve trillions of tokens, stand in stark contrast to the comparatively limited data humans are exposed to by the age of 12. This disparity has sparked studies drawing parallels between LLM training and human learning, exploring the amount of data exposure and its implications. The issue of efficiently retrieving facts and experiences from memory is another area under active investigation. Although Retrieval-Augmented Generation (RAG) presents a promising approach, achieving precise retrieval continues to be a research focus.</p><p name="a913" id="a913" class="graf graf--p graf-after--p">In upcoming posts, I plan to delve into advanced topics such as multi-agent interactions and why this is future of software architecture. Big tech is increasingly shifting from traditional software frameworks/architectures to those powered by LLMs. <a href="https://twitter.com/karpathy/status/1723140519554105733" data-href="https://twitter.com/karpathy/status/1723140519554105733" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Andrej Karpathy likens this shift to the emergence of an ‘LLM OS’</a>. This new paradigm integrates LLMs as the ‘brain’, databases for memory, sensors for perception, and motors or displays as actuators, culminating in a hardware-optimized LLM operating system. Additionally, Karpathy emphasizes the need for <a href="https://medium.com/@prdeepak.babu/reward-hacking-in-large-language-models-llms-c57abbc0cde7?source=your_stories_page-------------------------------------" data-href="https://medium.com/@prdeepak.babu/reward-hacking-in-large-language-models-llms-c57abbc0cde7?source=your_stories_page-------------------------------------" class="markup--anchor markup--p-anchor" target="_blank">robust security measure</a>s within LLM applications, particularly to safeguard against prompt injection and hacking attempts aimed at manipulating LLMs to perform unintended actions.</p><p name="1675" id="1675" class="graf graf--p graf-after--p">Related Posts</p><div name="a686" id="a686" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14" data-href="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14"><strong class="markup--strong markup--mixtapeEmbed-strong">Exploring the LLM Landscape: From Parametric Memory to Agent-Oriented Models</strong><br><em class="markup--em markup--mixtapeEmbed-em">LLMs are fast-evolving and we have a new model every week, showing up on leaderboards[1] beating previous SoTA model on…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="2a49296d6b021f39aedd533b1ea0d4c3" data-thumbnail-img-id="1*XSCmjkUswBC0I1PfZQQgXA.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*XSCmjkUswBC0I1PfZQQgXA.jpeg);"></a></div><div name="4ecd" id="4ecd" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://blog.gopenai.com/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0" data-href="https://blog.gopenai.com/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://blog.gopenai.com/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0"><strong class="markup--strong markup--mixtapeEmbed-strong">From N-grams to GPT-4: The Meteoric Rise of Large Language Models</strong><br><em class="markup--em markup--mixtapeEmbed-em">Large Language Models (LLMs) have caused a paradigm shift in the way NLP is traditionally done i.e anything to do with…</em>blog.gopenai.com</a><a href="https://blog.gopenai.com/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="ac46aa495334186ffed9a4e91f29c4b2" data-thumbnail-img-id="1*ChubSji0BNPU499IUSz-Rw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*ChubSji0BNPU499IUSz-Rw.png);"></a></div><div name="aec7" id="aec7" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed graf--trailing"><a href="https://medium.com/@prdeepak.babu/decoding-ais-thought-process-mechanistic-interpretability-54db5727e7e8" data-href="https://medium.com/@prdeepak.babu/decoding-ais-thought-process-mechanistic-interpretability-54db5727e7e8" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/decoding-ais-thought-process-mechanistic-interpretability-54db5727e7e8"><strong class="markup--strong markup--mixtapeEmbed-strong">Decoding AI’s Thought Process: Mechanistic Interpretability</strong><br><em class="markup--em markup--mixtapeEmbed-em">Mechanistic Interpretability is a field of study that concerns study of neural networks (more generally ML models) with…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/decoding-ais-thought-process-mechanistic-interpretability-54db5727e7e8" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8e21c192664b64e2d42feb22069de72d" data-thumbnail-img-id="0*-Z4XL91lYNGItrL0" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*-Z4XL91lYNGItrL0);"></a></div></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/the-dawn-of-synthetic-brains-how-large-language-models-are-shaping-ai-agents-640a78f3a830">https://medium.com/@prdeepak.babu/the-dawn-of-synthetic-brains-how-large-language-models-are-shaping-ai-agents-640a78f3a830</a></p>
