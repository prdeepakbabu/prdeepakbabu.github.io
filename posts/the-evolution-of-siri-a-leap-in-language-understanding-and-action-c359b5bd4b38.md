---
title: "The Evolution of Siri: A Leap in Language Understanding and Action"
date: "2024-06-09"
coverImage: "https://cdn-images-1.medium.com/max/1024/1*K9AGgPtTsVZxwoL6lS2rYQ.png"
canonical: "https://medium.com/@prdeepak.babu/the-evolution-of-siri-a-leap-in-language-understanding-and-action-c359b5bd4b38"
mediumId: "c359b5bd4b38"
source: "medium"
---

<section name="ecca" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="0a76" id="0a76" class="graf graf--p graf-after--h3">I might be biased, but it seems that Siri’s ability to understand and execute commands has recently taken a significant leap forward. This step-function improvement in Siri’s responses might be due to advancements in AI architectures, particularly generative language models (LLMs).</p><h3 name="6062" id="6062" class="graf graf--h3 graf-after--figure">A Real-World Example</h3><p name="c7cc" id="c7cc" class="graf graf--p graf-after--h3">Just the other day, I asked Siri to enable transparency mode on my Beats Studio headphones, using rather unconventional language. To my surprise, Siri understood and executed the command flawlessly. This level of understanding and flexibility in processing natural language appears to be a notable improvement from previous generations of voice assistants like Alexa, Siri, and Google Assistant, which often struggled with “unactionable” requests — those they weren’t explicitly designed to handle.</p><h3 name="f8c7" id="f8c7" class="graf graf--h3 graf-after--p">The Challenge of Unactionable Requests</h3><p name="18f2" id="18f2" class="graf graf--p graf-after--h3">In earlier iterations, pre-LLM voice assistants had difficulty distinguishing between actionable and unactionable commands. A typical response to an unactionable request would be a templated “Sorry, I don’t know that.” This was a significant limitation, as it often left users frustrated and questioning the utility of their voice assistants.</p><blockquote name="3064" id="3064" class="graf graf--blockquote graf-after--p">Pre-LLM voice assistants had difficulty distinguishing between actionable and unactionable commands.</blockquote><h3 name="b47d" id="b47d" class="graf graf--h3 graf-after--blockquote">The Impact of Generative LLMs</h3><p name="3256" id="3256" class="graf graf--p graf-after--h3">The current generation of voice assistant architecture, potentially powered by nuanced natural language understanding (NLU) through generative LLMs, seems to have drastically improved this situation. These models excel in understanding context, reasoning through language, and generating appropriate responses.</p><p name="f6be" id="f6be" class="graf graf--p graf-after--p">Consider another example: <strong class="markup--strong markup--p-strong">“Siri, add the currently playing song to my playlist vibecheck.”</strong> Siri’s response was, <strong class="markup--strong markup--p-strong">“YouTube Music hasn’t added support for this feature yet.”</strong> This response struck me as remarkably natural and fitting for several reasons:</p><ol class="postList"><li name="7c70" id="7c70" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Context</strong>: Siri accurately determined that the request pertained to the foreground app, YouTube Music.</li><li name="4224" id="4224" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Language Understanding &amp; Reasoning</strong>: Siri appeared to understand the intent behind the request and checked whether the necessary APIs or actions were available to support it.</li><li name="7157" id="7157" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Generation</strong>: Siri provided a natural language response that clearly communicated the lack of support for the requested feature.</li></ol><h3 name="e8d7" id="e8d7" class="graf graf--h3 graf-after--li">What’s Next for Voice Assistants?</h3><p name="dcbd" id="dcbd" class="graf graf--p graf-after--h3">As someone who has worked extensively in the field of AI and voice assistants, I’ve always been fascinated by their potential and the rapid advancements we’ve seen in recent years. The advancements in Siri’s capabilities suggest exciting possibilities for even more sophisticated interactions in the future. Here are a few speculative ideas:</p><h3 name="fb1d" id="fb1d" class="graf graf--h3 graf-after--p">Dynamic Feedback and Learning</h3><p name="4dad" id="4dad" class="graf graf--p graf-after--h3">Imagine if Siri could add, “…But that seems like a useful feature. I’ll pass on the feedback to the developers of the YouTube Music app.” This would not only enhance the user experience but also create a feedback loop that could drive app improvements based on user interactions.</p><h3 name="7ab9" id="7ab9" class="graf graf--h3 graf-after--p">Tool Learning Mechanisms</h3><p name="cd87" id="cd87" class="graf graf--p graf-after--h3">A more advanced possibility involves dynamically generating unavailable APIs and executing tasks behind the scenes. This concept, known as tool learning, is already being explored in various research papers, with <a href="https://arxiv.org/abs/2305.17126" data-href="https://arxiv.org/abs/2305.17126" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LATAM (Language Models as Tool Makers)</a> being one of the most notable examples. Tool learning mechanisms could enable voice assistants to perform a wider range of tasks by effectively “creating” the necessary tools on the fly.</p><blockquote name="8bd9" id="8bd9" class="graf graf--blockquote graf-after--p">Tool learning mechanisms could enable voice assistants to perform a wider range of tasks by effectively ‘creating’ the necessary tools on the fly</blockquote><h3 name="aa8a" id="aa8a" class="graf graf--h3 graf-after--blockquote">Conclusion</h3><p name="cecc" id="cecc" class="graf graf--p graf-after--h3">The recent improvements in Siri’s understanding and execution capabilities seem to indicate significant advancements in the field of AI and natural language processing. As generative LLMs continue to evolve, we might see voice assistants becoming even more intuitive, responsive, and capable of handling a broader range of requests. The future of voice-assisted technology appears bright, and the possibilities seem endless.</p><div name="5ec7" id="5ec7" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" data-href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f"><strong class="markup--strong markup--mixtapeEmbed-strong">The Rise of Multimodal Large Speech &amp; Language Models</strong><br><em class="markup--em markup--mixtapeEmbed-em">In the age of foundational models that are based on deep learning architectures like transformer models, we can process…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="d72a4f6e2a6a50e7d5e804cb84e9740a" data-thumbnail-img-id="0*NnNDl_c-ZTERdkjl" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*NnNDl_c-ZTERdkjl);"></a></div><div name="088d" id="088d" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed graf--trailing"><a href="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14" data-href="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14"><strong class="markup--strong markup--mixtapeEmbed-strong">Exploring the LLM Landscape: From Parametric Memory to Agent-Oriented Models</strong><br><em class="markup--em markup--mixtapeEmbed-em">LLMs are fast-evolving and we have a new model every week, showing up on leaderboards[1] beating previous SoTA model on…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="2a49296d6b021f39aedd533b1ea0d4c3" data-thumbnail-img-id="1*XSCmjkUswBC0I1PfZQQgXA.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*XSCmjkUswBC0I1PfZQQgXA.jpeg);"></a></div></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/the-evolution-of-siri-a-leap-in-language-understanding-and-action-c359b5bd4b38">https://medium.com/@prdeepak.babu/the-evolution-of-siri-a-leap-in-language-understanding-and-action-c359b5bd4b38</a></p>
