---
title: "From N-grams to GPT-4: The Meteoric Rise of Large Language Models"
date: "2023-04-20"
coverImage: "https://cdn-images-1.medium.com/max/1024/1*ChubSji0BNPU499IUSz-Rw.png"
canonical: "https://medium.com/@prdeepak.babu/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0"
mediumId: "d9e064ec7bf0"
source: "medium"
---

<section name="e47e" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="f270" id="f270" class="graf graf--p graf-after--h3">Large Language Models (LLMs) have caused a paradigm shift in the way NLP is traditionally done i.e anything to do with text. I have been working in this domain for 10+ years having seen n-gram ways of processing textual data getting replaced with word vectors or word embeddings (word2vec) and now transformer-based models like BERT, GPT and T5 taking this to an all new level. Around 2022, I started talking about this nascent technology, <a href="https://www.linkedin.com/in/prdeepak/overlay/1635506121042/single-media-viewer?type=DOCUMENT&amp;profileId=ACoAAAPmydABWOprFdlhrXaXAn5BiT-gpICb9yw&amp;lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BkYa5PIzIQxuKg5TLyX4yXQ%3D%3D" data-href="https://www.linkedin.com/in/prdeepak/overlay/1635506121042/single-media-viewer?type=DOCUMENT&amp;profileId=ACoAAAPmydABWOprFdlhrXaXAn5BiT-gpICb9yw&amp;lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BkYa5PIzIQxuKg5TLyX4yXQ%3D%3D" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">giving talks</a> to groups small and big to share my excitement towards emerging foundational LLM technology that worked strikingly similar to how humans grow and learn new concepts. Now it is 2023 and my excitement towards this tech has multiplied 10X to say the least.</p><p name="1a2e" id="1a2e" class="graf graf--p graf-after--p">I believe we have reached a critical juncture, a turning point, in the development of AGI, and we are living in exciting times. The next Google or next Facebook of the decade ahead, will be built by individuals and companies (small and large) taking baby steps today towards making use of these LLMs in innovative ways to build products and features. In terms of ML/AI advancements and the speed at which they are progressing, it seems that a single day in 2023 is equivalent to a month in 2020, and a year in 2010.</p><p name="5371" id="5371" class="graf graf--p graf-after--figure">I still remember reading the book <a href="https://www.amazon.com/Master-Algorithm-Ultimate-Learning-Machine-ebook/dp/B012271YB2" data-href="https://www.amazon.com/Master-Algorithm-Ultimate-Learning-Machine-ebook/dp/B012271YB2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">The Master Algorithm</a> by Pedro Domingos. The year was 2018. Pedro talks about a master algorithm — single AI model to rule all tasks. Today we have regression models for predicting continuous quantities and different classes of models to predict discrete or categorical data. For other complex data types like image, text and audio they have their own algorithms. As an example, audio data involves fourier decomposition, image data requires filter banks and so on. Alternately, these complex data types have often been collapsed into low-dimension representation which can then be used with traditional learning algorithms. My initial reaction to this book back in 2018 was “As an insider, Well this is too far from where we are and unlikely to happen in a decade!”. Fast Forward to 2021 — I was amazed when Google released T5 showing how it could do varied tasks like regression, classification, reasoning, language generation purely as text-to-text. What Pedro talked about in his book was starting to happen ! Since then, generative LLMs, like the most recent GPT-4 model by OpenAI, have grown in popularity, showing human-level performance on various tasks and even superhuman performance on some tasks.</p><p name="d1c8" id="d1c8" class="graf graf--p graf-after--p">LLMs like <a href="https://openai.com/product/gpt-4" data-href="https://openai.com/product/gpt-4" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GPT4</a> are seen to have <a href="https://arxiv.org/pdf/2206.07682.pdf" data-href="https://arxiv.org/pdf/2206.07682.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">emergent properties</a> that are absent in similar models of similar architecture. There have been studies that show how GPT-4 can learn to translate low-resource languages that have not been seen in model training with just a few examples as in-context learning. Similarly, they have shown ability to write complex code based on natural language instructions, do planning and reasoning to tasks outside its data regime. Jason Wei maintains an excellent repository of <a href="https://www.jasonwei.net/blog/emergence" data-href="https://www.jasonwei.net/blog/emergence" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">emergent behaviors</a> in LLMs like GPT and Chinchilla.</p><p name="9aaa" id="9aaa" class="graf graf--p graf-after--p">LLMs like GPT-3/4 are capable of complex reasoning which has opened up avenues to apply RL-like sequential decision making using LLMs as Agents which is more efficient than dealing with agents that are trained on large-scale dataset of simulated events and rewards as in traditional RL. This has led to development of role-playing LLM agents that continuously self-reflect and refine giving it autonomous capabilities. They are able to have very human-like conversations and engage in constructively debating on ideas and problems.</p><p name="bcf3" id="bcf3" class="graf graf--p graf-after--p">On the engineering side, OS tools like <a href="https://python.langchain.com/en/latest/index.html" data-href="https://python.langchain.com/en/latest/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">langchain</a> are building abstractions that are making SOTA research on LLMs and agents available to developers and scientists instantly and we have seen innovative applications like Auto-GPT and BabyAGI that are making AI beneficial for the masses. On a related note, LLMs are getting increasingly interdisciplinary with connections to Psychology involving “<a href="https://iep.utm.edu/theomind/" data-href="https://iep.utm.edu/theomind/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Theory of Mind</a>“</p><p name="d047" id="d047" class="graf graf--p graf-after--p">While there are many open scientific questions about how LLMs accomplish their tasks. I feel following topics are important to be understood</p><ul class="postList"><li name="b7e5" id="b7e5" class="graf graf--li graf-after--p">Can we separate “language understanding” from “knowledge”. Today’s LLMs like GPT, PaLM, GPT do not distinguish these two and to some extent it is hard and ambiguous. I feel if we can solve this, we can have much smaller LLMs that are good at language understanding so we can provide plug-and-play KBs to effectively reason. We have seen just reducing the 175B model to 7B or 11B model makes emergent properties disappear without this distinction.</li><li name="2098" id="2098" class="graf graf--li graf-after--li">How can we leverage LLMs like GPT or Chinchilla in latency-sensitive applications like Ads and Search in a cost-effective manner.</li><li name="ab96" id="ab96" class="graf graf--li graf-after--li">Does “Data” beat the model “Size” ? Studies on scaling law attempt to arrive at the sweet spot between data, size and compute.</li></ul><p name="2928" id="2928" class="graf graf--p graf-after--li graf--trailing">We are surely living in interesting times. Motivated by all the progress and tireless follow-up to stay relevant in LLMs progress. I will be starting an LLM series with a plan to publish interesting topics — research, applied and trends to closely watch. This will include reviewing promising papers, tools, research, applications and ideas.Let me know what topics you would like to hear about in the comments.</p></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0">https://medium.com/@prdeepak.babu/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0</a></p>
