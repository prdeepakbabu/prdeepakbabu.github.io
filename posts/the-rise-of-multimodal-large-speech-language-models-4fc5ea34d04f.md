---
title: "The Rise of Multimodal Large Speech & Language Models"
date: "2023-12-04"
coverImage: "https://cdn-images-1.medium.com/max/1024/0*NnNDl_c-ZTERdkjl"
canonical: "https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f"
mediumId: "4fc5ea34d04f"
source: "medium"
---

<section name="3b6a" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="53c3" id="53c3" class="graf graf--p graf-after--h3">In the age of foundational models that are based on deep learning architectures like transformer models, we can process large amounts of data to learn complex representations of data, forming world knowledge towards achieving Artificial General Intelligence (AGI). Most developments in foundation models until 2022 have revolved around text data, and we understand how to use web-scale unlabelled text data to train <a href="https://medium.com/gopenai/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0" data-href="https://medium.com/gopenai/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0" class="markup--anchor markup--p-anchor" target="_blank">Large Language Models (LLMs)</a> in a self-supervised way. This training generates text that shows signs of intelligence, planning, and reasoning. However, by some estimates, we are running out of text data and hitting a ceiling according to the scaling laws of LLMs. Interacting with LLMs purely through text can be limiting in many cases. There are many other structured modalities that encode information difficult to capture through text. For example, audio can encode a wide range of emotions in a person’s speech, and images can represent the geometry and location of objects, which might be much harder to describe through text</p><blockquote name="49aa" id="49aa" class="graf graf--blockquote graf-after--p">What’s next? Humans perceive and respond to the world not just through language but also by taking cues from vision, hearing (speech), touch, and smell (olfaction). Traditionally, we have approached the perception problem by converting every aspect of a context into textual format, i.e., describing a scene in text and then conducting Q&amp;A based on that text. This method obviously leads to a loss of information. For example, in responding to a question, there might be many relevant aspects of an image that are not easily describable in text. This limitation motivates the need for Large Language Models (LLMs) to extend beyond text and incorporate vision, speech, touch, and smell. There is already an abundance of literature and models on text-plus-vision multimodal models. However, there is less discussion about how voice or speech can play a central role in these multimodal Foundation Models.</blockquote><figure name="bd0a" id="bd0a" class="graf graf--figure graf--iframe graf-after--blockquote"></figure><h3 name="0b0d" id="0b0d" class="graf graf--h3 graf-after--figure">Why Multimodality is important in Foundation LLM models ?</h3><p name="a421" id="a421" class="graf graf--p graf-after--h3">In the realm of artificial intelligence, the shift towards multimodal models isn’t just a technological trend — it’s a strategic move towards creating more human-like, efficient, and resourceful AI systems. Here’s why embracing multimodal approaches is vital:</p><ul class="postList"><li name="ceb1" id="ceb1" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Mimicking Human Perception</strong>: Humans don’t experience the world through text alone; our interactions are a rich tapestry of sights, sounds, and sensations. Multimodal Large Language Models (LLMs) strive to mimic this human-like perception by integrating multiple senses — visual, auditory, and beyond. This approach enables AI to interpret and respond to a broader spectrum of human communication, making interactions more natural and intuitive.</li><li name="3cd6" id="3cd6" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Sample and Data Efficiency</strong>: Multimodal models can achieve higher levels of accuracy and understanding with less data compared to unimodal systems. By leveraging multiple types of data — such as visual cues in addition to spoken words — these models can grasp the context and nuances of communication more effectively. This efficiency is particularly valuable in scenarios where one type of data is limited or ambiguous, as the additional modalities can provide complementary information.</li><li name="fe45" id="fe45" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Abundance of Multimodal Data</strong>: While the reservoir of text data might be plateauing, the world is awash in a sea of multimodal data. Every moment, countless images, videos, and audio recordings are created, offering a rich source of diverse data for AI models to learn from. This abundance not only provides a wealth of training material but also reflects the real-world scenarios where AI is expected to operate. By tapping into this vast multimodal dataset, AI systems can continually evolve and adapt to new challenges and contexts.</li><li name="1625" id="1625" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Beyond the Limitations of Text</strong>: Text, while immensely informative, has its limitations in conveying the full spectrum of human expression. Emotions, for instance, are often more palpably expressed through tone of voice or facial expressions than through words alone. Multimodal models can capture these subtle yet critical aspects of communication, leading to more empathetic and effective AI interactions. This capability is particularly crucial in fields like customer service, healthcare, and education, where understanding and responding to emotional cues can significantly impact the effectiveness of AI solutions.</li></ul><h3 name="7534" id="7534" class="graf graf--h3 graf-after--li"><strong class="markup--strong markup--h3-strong">Speech as Modality in LLMs</strong></h3><p name="3bbe" id="3bbe" class="graf graf--p graf-after--h3">I’d like to delve into the realm of speech in the context of AI, a domain that, unlike its counterpart in vision-language models, remains relatively underexplored in audio-language models that jointly model speech and language. Speech represents a rich tapestry of human intent, encompassing not just the spoken word but also capturing the nuances of the speaker and the surrounding environment. Consider the vibrancy of a crowded mall or the echoes of a large stadium; these auditory landscapes, along with the ability to discern speaker characteristics like age and gender, demonstrate the multifaceted nature of speech.</p><p name="755e" id="755e" class="graf graf--p graf-after--p">Historically, the fields of vision, NLP (text), and speech have operated in silos, each with its specialized conferences and research focus. Computer vision research gravitated towards CNNs, NLP found solace in RNNs and LSTMs, while the speech domain was immersed in the intricacies of HMMs and FSTs. However, a transformative wave arrived with the emergence of transformers, a robust deep learning architecture that began bridging these once disparate realms. This convergence has paved the way for the rise of multimodal LLMs, or LMMs (large multimodal models), which promise an integrated approach to understanding and interacting with our world.</p><p name="cbea" id="cbea" class="graf graf--p graf-after--p">An intriguing aspect of this integration lies in the similarity between speech and vision. Speech, when represented as a mel spectrogram, transcends its auditory boundaries to become a visual entity, akin to an image. This transformation allows it to be processed using computer vision algorithms, blurring the lines between hearing and seeing. Such interplay between modalities not only showcases the versatility of AI but also mirrors the multifaceted way humans perceive and interact with their surroundings. The journey into multimodal AI is not just a technological advancement; it’s a step closer to mirroring the rich sensory experience of human existence.</p><h3 name="982d" id="982d" class="graf graf--h3 graf-after--figure">The McGurk Effect</h3><p name="6880" id="6880" class="graf graf--p graf-after--h3"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4091305/" data-href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4091305/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">The McGurk Effect</a> offers a fascinating glimpse into the complexities of human speech perception, where what we see can significantly alter what we hear. It occurs when auditory and visual components of a speech signal are mismatched, leading to a third, different percept. For example, if the visual component of a person saying “ga-ga” is paired with the audio of them saying “ba-ba”, many people will perceive a third sound, like “da-da” or “tha-tha”. This phenomenon, where a visual stimulus (like lip movements) can change our auditory perception, underscores the importance of integrating multiple sensory modalities for effective communication. In the realm of multimodal foundation models, especially those involving speech (Multimodal Large Language Models or LMMs), this effect becomes particularly pertinent. It highlights the necessity for these advanced AI systems to not only process textual and auditory data in isolation but to integrate visual cues, such as lip movements or facial expressions, to accurately interpret and generate human-like responses. The McGurk Effect thus serves as a reminder of the intricacies of human sensory integration and offers valuable insights for developing more sophisticated, context-aware multimodal speech processing models that mimic this human ability, enhancing the models’ effectiveness in real-world interactions.</p><figure name="a1c5" id="a1c5" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/2k8fHR9jKVM" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="e412" id="e412" class="graf graf--p graf-after--figure">In a subsequent post (Part II), I will explore <a href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" data-href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" class="markup--anchor markup--p-anchor" target="_blank">state-of-the-art speech multimodal models</a> and their applications. Meanwhile, I hope this post has successfully highlighted the need for multimodality in today’s foundation models, which are predominantly language or text-based.</p><p name="ece3" id="ece3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Related Posts</strong></p><div name="946e" id="946e" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" data-href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8"><strong class="markup--strong markup--mixtapeEmbed-strong">Multimodal Mastery: The Qwen Audio Foundation Models for Advanced Audio Understanding and Reasoning</strong><br><em class="markup--em markup--mixtapeEmbed-em">As a follow-up to the last blog on large multimodal audio models (LMM), we’re here to explore an open-source large LMM…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="dd617b64c8c9efa6db69056c6e1bc8de" data-thumbnail-img-id="1*GZ-3hMim6LtmAiRUGeWSXw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*GZ-3hMim6LtmAiRUGeWSXw.png);"></a></div><div name="5649" id="5649" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed graf--trailing"><a href="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14" data-href="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14"><strong class="markup--strong markup--mixtapeEmbed-strong">Exploring the LLM Landscape: From Parametric Memory to Agent-Oriented Models</strong><br><em class="markup--em markup--mixtapeEmbed-em">LLMs are fast-evolving and we have a new model every week, showing up on leaderboards[1] beating previous SoTA model on…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="2a49296d6b021f39aedd533b1ea0d4c3" data-thumbnail-img-id="1*XSCmjkUswBC0I1PfZQQgXA.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*XSCmjkUswBC0I1PfZQQgXA.jpeg);"></a></div></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f">https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f</a></p>
