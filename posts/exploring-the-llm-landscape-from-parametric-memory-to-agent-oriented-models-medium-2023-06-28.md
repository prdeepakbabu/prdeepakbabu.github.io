---
title: "Exploring the LLM Landscape: From Parametric Memory to Agent-Oriented Models"
date: "2023-06-28"
coverImage: "https://cdn-images-1.medium.com/max/1024/1*XSCmjkUswBC0I1PfZQQgXA.jpeg"
canonical: "https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14"
mediumId: "ab0088d1f14"
source: "medium"
---

<section name="9a10" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="123d" id="123d" class="graf graf--p graf-after--figure">LLMs are fast-evolving and we have a new model every week, showing up on leaderboards[1] beating previous SoTA model on multiple NLP benchmarks. There are multiple architectures with nuances to training and dataset generation. This post is an attempt to broadly categorize the LLMs and paint a picture of different ways to adopt LLMs to use-cases, discuss pros and cons of each approach. It is to be noted, the distinction among these categories of LLMs are not crystal-clear and they have blurred boundary lines. As an example, you could have parameteric LLM as an agent or parameteric LLM trained using instruct format.</p><h3 name="b5aa" id="b5aa" class="graf graf--h3 graf-after--p">Parametric Memory LLMs: Self-Reliant Titans of Information</h3><p name="bba3" id="bba3" class="graf graf--p graf-after--h3">Parametric memory LLMs are akin to colossal knowledge repositories, perfectly encapsulating a world of information within their intricate neural network structures. These self-reliant models, devoid of external memory dependencies, can effectively store and retrieve knowledge through fixed weights within the network. This unique trait enables them to scale seamlessly with increasing parameter counts, thus achieving state-of-the-art accuracies on many NLI/U tasks.</p><p name="ad5b" id="ad5b" class="graf graf--p graf-after--p">Prominent examples of these titan models include Googleâ€™s PaLM (Pathway LM) with a staggering 540B parameters, GPT-3 housing 175B parameters, and the relatively lighter Chinchila[2] carrying 65B parameters. Though their monumental size poses challenges for inference, the AI community has responded with dexterity. Weâ€™re witnessing a promising trend of smaller, yet powerful models that match their larger counterparts in performance, thanks to strategic techniques like self-instruct and parameter-efficient training. Nonetheless, these models do possess their unique quirks, such as their propensity to â€˜hallucinateâ€™ or fabricate compelling yet false factsâ€Šâ€”â€Ša challenge yet to be overcome.</p><p name="734e" id="734e" class="graf graf--p graf-after--p">ğŸ‘ No dependency on external modules. Knowledge stored in parameter. benefits reasoning.</p><p name="6b93" id="6b93" class="graf graf--p graf-after--p">ğŸ‘ Tendency to hallucinate since knowledge is hidden in parameters and has no way to verify</p><p name="eb2e" id="eb2e" class="graf graf--p graf-after--p">ğŸ‘ Updating a model to recent events and facts requires full pretraining which is expensive</p><h3 name="72a5" id="72a5" class="graf graf--h3 graf-after--p">Non-Parametric or External Memory LLMs: Harnessing External Memory forÂ Freedom</h3><p name="0502" id="0502" class="graf graf--p graf-after--h3">In contrast to their parametric counterparts, non-parametric LLMs ingeniously leverage external memory resources, liberating themselves from the constraints of their internal memory. This innovative approach allows these models to remain streamlined and current without necessitating constant retraining and gradient updatesâ€Šâ€”â€Ša significant advantage that drastically reduces model hallucinations and ensures more reliable outputs.</p><p name="6c8b" id="6c8b" class="graf graf--p graf-after--p">However, every innovation brings its own set of challenges. In this instance, the added complexity of maintaining a supplementary retrieval model is an inevitable trade-off. Weâ€™re exploring various paradigms to manage this complexity, including â€˜frozenâ€™ LLM and plug-and-play KB techniques, and the ground-breaking Retrieval Augmented Generation (RAG) approach.</p><p name="ebee" id="ebee" class="graf graf--p graf-after--p">ğŸ‘ Reduced Hallucination. Can be smaller LLMs, knowledge outsourced to exernal memory fetch at inference</p><p name="dc5e" id="dc5e" class="graf graf--p graf-after--p">ğŸ‘ LLMs can preserve freshness without retraining, since knowledge is decoupled in form of external index</p><p name="7776" id="7776" class="graf graf--p graf-after--p">ğŸ‘ Makes the architecture complex as it needs a retrieval from external index.</p><h3 name="2a1a" id="2a1a" class="graf graf--h3 graf-after--p">LLM as Agents: A New Age of Reasoning andÂ Control</h3><p name="e9da" id="e9da" class="graf graf--p graf-after--h3">The AI research landscape is abuzz with an emerging and exciting prospectâ€Šâ€”â€ŠLLMs as autonomous agents proficient in planning and control. The concept of an LLM agent capable of breaking down complex tasks into component questions and actions has unleashed a world of possibilities.</p><p name="f80e" id="f80e" class="graf graf--p graf-after--p">Innovations such as ReACTâ€™s[3] LLM agent, or the various interpretations presented by Toolformer, ReACT, WebGPT, and DSP, are illuminating the way forward. These trailblazers are setting the stage for LLMs that can emulate human-like reasoning, invoke complex tools like Python code interpreters or mathematical calculators, and align with human values for a more dependable and meaningful response.</p><p name="15f9" id="15f9" class="graf graf--p graf-after--p">ğŸ‘ Human-like. Outsources things that are hard for LLM to do. Significantly reduces hallucinations. For math uses calculators, for puzzles can invoke python interpreter or other models.</p><p name="0466" id="0466" class="graf graf--p graf-after--p">ğŸ‘ Improved reasoning giving the LLM ability to accomplish task with super-human performance pushing LLMs towards AGI</p><p name="1b99" id="1b99" class="graf graf--p graf-after--p">ğŸ‘ Needs all tools available as API. In case of large tools, can run into context length limitations forcing SFT which can be expensive to collect data.</p><h3 name="c38b" id="c38b" class="graf graf--h3 graf-after--p">Instruct Models: Charting New Courses with Human Instructions</h3><p name="fc9a" id="fc9a" class="graf graf--p graf-after--h3">Instruct models[4], though not a distinct architecture, are causing quite a stir. They represent a novel data paradigm that is revolutionizing our interaction with LLMs. By formulating tasks as human instructions and fine-tuning LLMs to heed these directives, we have been able to create versatile models. These models not only generalize across tasks without explicit programming but also maintain alignment with human expectations and valuesâ€Šâ€”â€Šan exciting prospect for the future of AI.</p><p name="2575" id="2575" class="graf graf--p graf-after--p">Needless to say, LLMs are changing reference architectures for search/recommendation engines, databases, web and app development with components like vector DB, ensemble of LLMs, tools/API, prompt engine, etc. becoming a central piece in the software stack of a production system. We will see this accelerate in next few months and new reference architectures evolve. I anticipate there will be new areas emerging related to latency optimization, cost/token optimization, training and inference which are typically topics considered after-thought to take a centre stage.</p><p name="3164" id="3164" class="graf graf--p graf-after--p">What are you working onÂ ? what LLM and software architectures are you consideringÂ ? Feel free share it in comments.</p><p name="922c" id="922c" class="graf graf--p graf-after--p graf--trailing">[1] <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" data-href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a><br>[2] <a href="https://arxiv.org/abs/2203.15556" data-href="https://arxiv.org/abs/2203.15556" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/abs/2203.15556</a><br>[3] <a href="https://arxiv.org/abs/2210.03629" data-href="https://arxiv.org/abs/2210.03629" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/abs/2210.03629</a><br>[4] <a href="https://arxiv.org/abs/2109.01652" data-href="https://arxiv.org/abs/2109.01652" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/abs/2109.01652</a></p></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14">https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14</a></p>
