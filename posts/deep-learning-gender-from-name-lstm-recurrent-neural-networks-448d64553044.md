---
title: "Deep learning gender from name -LSTM Recurrent Neural Networks"
date: "2017-04-23"
canonical: "https://medium.com/@prdeepak.babu/deep-learning-gender-from-name-lstm-recurrent-neural-networks-448d64553044"
mediumId: "448d64553044"
source: "medium"
---

<section name="e83e" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><div name="c591" id="c591" class="graf graf--mixtapeEmbed graf-after--h3"><a href="https://github.com/prdeepakbabu/Python/tree/master/Deep%20learning%20gender" data-href="https://github.com/prdeepakbabu/Python/tree/master/Deep%20learning%20gender" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/prdeepakbabu/Python/tree/master/Deep%20learning%20gender"><strong class="markup--strong markup--mixtapeEmbed-strong">prdeepakbabu/Python</strong><br><em class="markup--em markup--mixtapeEmbed-em">Python - All *.py scripts</em>github.com</a><a href="https://github.com/prdeepakbabu/Python/tree/master/Deep%20learning%20gender" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="30ce4a69bd9fd4f405a8ba1794d22c4d" data-thumbnail-img-id="0*xKU3URt2L-r11XbC." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*xKU3URt2L-r11XbC.);"></a></div><p name="6768" id="6768" class="graf graf--p graf-after--mixtapeEmbed">Deep learning neural networks have shown promising results in problems related to vision, speech and text with varying degrees of success. I have tried looking at a text problem here, where we are trying to predict gender from name of the person. RNNs are a good fit for this as it involves learning from sequences (in this case sequence of characters). Traditional RNNs have learning problems due to vanishing gradients. Recent advancements have shown two variants of RNN can help solve the problem</p><p name="ccbb" id="ccbb" class="graf graf--p graf-after--p">(i) LSTM or Long short term memory- uses memory/forget gates to retain or pass patterns learnt in sequence useful for predicting target variable. we will use this for our model. (recommend <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" data-href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">colah’s blog</a> for a deeper understanding of theory behind LSTM)</p><p name="09f5" id="09f5" class="graf graf--p graf-after--p">(ii) GRU or gated recurrent unit</p><p name="fa91" id="fa91" class="graf graf--p graf-after--p">To formally state the problem, we are interested in predicting if the given name is male/female. In the past, there have been attempts to predict gender based on simple rules on name as seen in NLTK, for example that relies on last character in name to classify gender which suffers from poor accuracy due to high generalization.</p><p name="547d" id="547d" class="graf graf--p graf-after--p">we will use character sequences which make up the name as our X variable, with Y variable as m/f indicating the gender. we use a stacked LSTM model and a final dense layer with softmax activation (many-to-one setup). categorical cross-entropy loss is used with adam optimizer. A 20% dropout layer is added for regularization to avoid over-fitting. The schematic shows the model setup.</p><figure name="d688" id="d688" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Hgbd2rTsr8hibGpWZ72mhQ.jpeg" data-width="3264" data-height="1892" src="https://cdn-images-1.medium.com/max/800/1*Hgbd2rTsr8hibGpWZ72mhQ.jpeg"><figcaption class="imageCaption">LSTM RNN Architecture.</figcaption></figure><p name="ac30" id="ac30" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">About the dataset</strong></p><p name="a05e" id="a05e" class="graf graf--p graf-after--p"><a href="https://gist.github.com/mbejda/9b93c7545c9dd93060bd" data-href="https://gist.github.com/mbejda/9b93c7545c9dd93060bd" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">W</a>e use indian names dataset available in <a href="https://gist.github.com/mbejda/9b93c7545c9dd93060bd" data-href="https://gist.github.com/mbejda/9b93c7545c9dd93060bd" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">mbejda github</a> account which has a collection of male and female indian name database collected from public records. Basic preprocessing is done to remove duplicates, special characters. Final distribution of m/f classes is 55%:45%. It is to be noted we use the full name here. some names have more than 2 words depending on surname, family name,etc.</p><p name="0770" id="0770" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Implementation using keras</strong></p><p name="838a" id="838a" class="graf graf--p graf-after--p">The complete dataset, code and python notebook is avaialble in my <a href="https://github.com/prdeepakbabu/Python/tree/master/Deep%20learning%20gender" data-href="https://github.com/prdeepakbabu/Python/tree/master/Deep%20learning%20gender" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">github repo</a>.The complete implementation is done using keras with tensorflow backend. Input representations &amp; parameters are highlighted below</p><p name="dbd0" id="dbd0" class="graf graf--p graf-after--p">(i) <em class="markup--em markup--p-em">vocabulary </em>: we have a set of 39 chars including a-z, 0–9, space, dot and a special END token.<br>(ii) <em class="markup--em markup--p-em">max sequence length</em>: is chosen as 30 ie. chars exceeding 30 chars are truncated. if name has less than 30 chars “END” token is padded.<br>(iii) <em class="markup--em markup--p-em">one hot encoding</em>: each of the character is one-hot encoded represented as [1 X 39] dimension array.<br>(iv) <em class="markup--em markup--p-em">batch size</em>: 1000 samples in a batch<br>(v) <em class="markup--em markup--p-em">epochs </em>: 50 epochs or 50 times we iterate over the entire dataset (see once)<br>(vi)<em class="markup--em markup--p-em">Y labels </em>: represented as array of [1 X 2] with first column indicating male, second col for female. ex: [1 0] for male.</p></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="fb71" id="fb71" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 51.822%;"><img class="graf-image" data-image-id="1*NbTBXKYQG1ah3ymzIE1DAQ.png" data-width="641" data-height="289" src="https://cdn-images-1.medium.com/max/800/1*NbTBXKYQG1ah3ymzIE1DAQ.png"></figure><figure name="9e01" id="9e01" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 48.178%;"><img class="graf-image" data-image-id="1*kMmpGDPzODjrRbq5WGKdPA.png" data-width="730" data-height="354" src="https://cdn-images-1.medium.com/max/600/1*kMmpGDPzODjrRbq5WGKdPA.png"><figcaption class="imageCaption" style="width: 207.564%; left: -107.564%;">Loss &amp; Accuracy charts as a function of epochs. As seen from loss charts, after 40 epochs the validation loss saturates while training loss keep reducing indicating over-fitting problems.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="808b" id="808b" class="graf graf--p graf-after--figure">The model took ~ 2 hours to run on my moderate high end laptop (CPU based). The final <strong class="markup--strong markup--p-strong">classification accuracy stands at 86%</strong> on validation dataset. More stats around precision,recall and confusion matrix is presented below.</p><figure name="3f23" id="3f23" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Qng3WJuRbJwldz6hbuIWIA.png" data-width="298" data-height="101" src="https://cdn-images-1.medium.com/max/800/1*Qng3WJuRbJwldz6hbuIWIA.png"></figure><p name="d474" id="d474" class="graf graf--p graf-after--figure">Model shows a higher precision for identifying male (88%) vs. female(84%). Of the total 3,028 samples in validation, 411 samples were incorrectly predicted leading to accuracy of 86.43% and error of 13.5%</p><p name="4824" id="4824" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Interpreting the Model (what did the model learn ?)</strong></p><p name="f0c2" id="f0c2" class="graf graf--p graf-after--p">looking at the predictions made on validation set, the model seems to have learnt following patterns in sequences. For example occurance“mr.” implies a likely male name, while occurance of “smt” and “mrs” are indicative of female names. The beauty of deep learning is about end to end learning capability without needing an explicit feature engineering step needed in traditional learning. Additionally female names tend to end with vowels a,i,o which model seems to have picked. There are more complex patterns picked up by model, not easily visible through inspection.</p><figure name="24b8" id="24b8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yUkJZljRifyM9vbCik4KEg.png" data-width="456" data-height="101" src="https://cdn-images-1.medium.com/max/800/1*yUkJZljRifyM9vbCik4KEg.png"></figure><p name="df0f" id="df0f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Next steps</strong></p><p name="1835" id="1835" class="graf graf--p graf-after--p">Some thoughts on improving the model accuracy are</p><p name="d539" id="d539" class="graf graf--p graf-after--p">(i) Using pre-trained character embedding instead of one-hot encoding (like <a href="https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt" data-href="https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this</a>). Remember this quote applies to characters equally</p><blockquote name="6017" id="6017" class="graf graf--blockquote graf-after--p">You shall know a word by the company it keeps</blockquote><p name="883b" id="883b" class="graf graf--p graf-after--blockquote">(ii) Better sampling<br> a. limit to first name and last name. <br> b. sub-sample male class to balance m-f classes as 1:1.<br> c. remove all non-chars from vocabulary.<br>(iii) hyper-parameter tuning<br>a. max length of sequence<br>b. stacked layers.<br>c. LSTM gates</p><p name="96b4" id="96b4" class="graf graf--p graf-after--p graf--trailing">I would love to hear your comments / questions / suggestions regarding the post, please share if you find this interesting.</p></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/deep-learning-gender-from-name-lstm-recurrent-neural-networks-448d64553044">https://medium.com/@prdeepak.babu/deep-learning-gender-from-name-lstm-recurrent-neural-networks-448d64553044</a></p>
