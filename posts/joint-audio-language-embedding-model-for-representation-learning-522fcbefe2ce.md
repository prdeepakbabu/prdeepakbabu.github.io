---
title: "Joint Audio-Language Embedding Model for Representation Learning"
date: "2024-01-22"
coverImage: "https://cdn-images-1.medium.com/max/1024/1*_pw5n7bSRCJtF_lVGbEarw.png"
canonical: "https://medium.com/@prdeepak.babu/joint-audio-language-embedding-model-for-representation-learning-522fcbefe2ce"
mediumId: "522fcbefe2ce"
source: "medium"
---

<section name="d249" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="8e41" id="8e41" class="graf graf--p graf-after--h3">We have been discussing audio and language models in last few posts (<a href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" data-href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" class="markup--anchor markup--p-anchor" target="_blank">Multimodal audio-language LMM</a> and <a href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" data-href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" class="markup--anchor markup--p-anchor" target="_blank">Qwen-Audio FM</a>). In continuation to these, here we discuss another important foundation model for representation learning of audio (or rather joint representation of audio+text) which unlocks new capabilities like zero-shot audio classification and audio retrieval, mimicing its image counterparts in image classification and retrieval. Together Qwen-Audio and CLAP, both open source audio foundation models caters to a wide range of audio use-cases from reasoning, understanding to classification.</p><h3 name="5177" id="5177" class="graf graf--h3 graf-after--p">CLAP Contrastive Language Audio Pretraining</h3><p name="2a28" id="2a28" class="graf graf--p graf-after--h3">CLIP was an early vision-language foundation model that used contrastive learning to align vision and text modality using image captioning as the paired data for alignment. Once aligned, both vision and language embeddings learnt by the model are in same space and hence allowed interesting vector arithmetic between images and language resulting in cool applications like image retrieval and zero-shot classification of images.</p><p name="b9fa" id="b9fa" class="graf graf--p graf-after--figure">Motivated by CLIP, Microsft cross-polinated the idea of CLIP to aligning audio and language modalities using audio classification data as paired data for alignemnt. The audio and language encoders are now in same shared space to allow vector arithmetic between audio and text. This opens up applications involving (but not limited to)</p><ul class="postList"><li name="ee64" id="ee64" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Audio Retrieval </strong>— Use text description to retrieve relevant audio. Say you want all audio files which have train sound. You could index all your audio files using its CLAP embedding and find the top-K nearest neigbhors for the CLAP embedding (“sound of train”).</li><li name="c40d" id="c40d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Audio Classification</strong> — zero-shot audio classification. Say you want to identify if the audio belongs to sound of train, bus, flight, bike or a boat. With no supervised data, you could use CLAP embeddings to find cosine similarity between audio embedding and class label text embedding (i.e “sound of bus”, “sound of train”, etc.) The class with the highest probability decides the class label for the audio in question.</li></ul><p name="0d91" id="0d91" class="graf graf--p graf-after--li">Below is a script that loads <a href="https://arxiv.org/pdf/2211.06687.pdf" data-href="https://arxiv.org/pdf/2211.06687.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CLAP (a variant called LS-CLAP large scale</a>) and run inference on a train sound. The model identifies the majority label to be “train” with a high prob. of 99.87%. It is to be noted, this is zero-shot classification i.e we havent trained or fine-tuned the model with any specific sounds.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="9f15" id="9f15" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment">#import packages</span><br /><span class="hljs-keyword">import</span> datasets<br /><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio<br /><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br /><br /><span class="hljs-comment">#read audio file</span><br />audio_dataset = datasets.Dataset.from_dict({<span class="hljs-string">&quot;audio&quot;</span>: [<span class="hljs-string">&quot;train_journey_sound.wav&quot;</span>]}).cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio())<br />audio=audio_dataset[<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;array&quot;</span>]<br /><br /><span class="hljs-comment">#define_class_labels</span><br />y_labels = [<br /><span class="hljs-string">&quot;Sound of train&quot;</span>,<br /><span class="hljs-string">&quot;Sound of bus&quot;</span>,<br /><span class="hljs-string">&quot;Sound of flight&quot;</span>,<br /><span class="hljs-string">&quot;Sound of bike&quot;</span>,<br /><span class="hljs-string">&quot;Sound of boat&quot;</span><br />]<br /><br /><span class="hljs-comment">#import CLAP model and run inference (zero-shot)</span><br />audio_classifier = pipeline(task=<span class="hljs-string">&quot;zero-shot-audio-classification&quot;</span>, model=<span class="hljs-string">&quot;audio_models/larger_clap_music_and_speech&quot;</span>)<br />output = audio_classifier(audio, candidate_labels=y_labels)<br /><span class="hljs-built_in">print</span>(output)</span></pre><p name="9027" id="9027" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Output</strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="45ad" id="45ad" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">[{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9987432360649109</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;Sound of train&#x27;</span>}, <br />{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.0011509136529639363</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;Sound of bike&#x27;</span>}, <br />{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">5.262664853944443e-05</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;Sound of boat&#x27;</span>}, <br />{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">3.159555490128696e-05</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;Sound of bus&#x27;</span>}, <br />{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">2.1611924239550717e-05</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;Sound of flight&#x27;</span>}]</span></pre><p name="a6c6" id="a6c6" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Related Posts</strong></p><div name="e561" id="e561" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" data-href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8"><strong class="markup--strong markup--mixtapeEmbed-strong">Multimodal Mastery: The Qwen Audio Foundation Models for Advanced Audio Understanding and Reasoning</strong><br><em class="markup--em markup--mixtapeEmbed-em">As a follow-up to the last blog on large multimodal audio models (LMM), we’re here to explore an open-source large LMM…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="dd617b64c8c9efa6db69056c6e1bc8de" data-thumbnail-img-id="1*GZ-3hMim6LtmAiRUGeWSXw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*GZ-3hMim6LtmAiRUGeWSXw.png);"></a></div><div name="9cf2" id="9cf2" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" data-href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f"><strong class="markup--strong markup--mixtapeEmbed-strong">The Rise of Multimodal Large Speech &amp; Language Models</strong><br><em class="markup--em markup--mixtapeEmbed-em">In the age of foundational models that are based on deep learning architectures like transformer models, we can process…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="d72a4f6e2a6a50e7d5e804cb84e9740a" data-thumbnail-img-id="0*NnNDl_c-ZTERdkjl" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*NnNDl_c-ZTERdkjl);"></a></div><div name="812b" id="812b" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed graf--trailing"><a href="https://medium.com/@prdeepak.babu/the-dawn-of-synthetic-brains-how-large-language-models-are-shaping-ai-agents-640a78f3a830" data-href="https://medium.com/@prdeepak.babu/the-dawn-of-synthetic-brains-how-large-language-models-are-shaping-ai-agents-640a78f3a830" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/the-dawn-of-synthetic-brains-how-large-language-models-are-shaping-ai-agents-640a78f3a830"><strong class="markup--strong markup--mixtapeEmbed-strong">The Dawn of Synthetic Brains: How Large Language Models Are Shaping AI Agents</strong><br><em class="markup--em markup--mixtapeEmbed-em">Recent advancements in Large Language Models (LLMs) have demonstrated a significant capability for reasoning and…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/the-dawn-of-synthetic-brains-how-large-language-models-are-shaping-ai-agents-640a78f3a830" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="28eac622b9179426596513720f0d6363" data-thumbnail-img-id="1*T6XlrGGcdwnZTrhPRs3ENA.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*T6XlrGGcdwnZTrhPRs3ENA.png);"></a></div></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/joint-audio-language-embedding-model-for-representation-learning-522fcbefe2ce">https://medium.com/@prdeepak.babu/joint-audio-language-embedding-model-for-representation-learning-522fcbefe2ce</a></p>
