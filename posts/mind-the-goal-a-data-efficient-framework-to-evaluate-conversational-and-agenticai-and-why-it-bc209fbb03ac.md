---
title: "Mind the Goal: A Data-Efficient Framework to Evaluate Conversational and AgenticAI (and Why It…"
date: "2025-11-19"
coverImage: "https://cdn-images-1.medium.com/max/1024/1*MPUsCdGvKIO72L7ZzdFAXg.png"
canonical: "https://medium.com/@prdeepak.babu/mind-the-goal-a-data-efficient-framework-to-evaluate-conversational-and-agenticai-and-why-it-bc209fbb03ac"
mediumId: "bc209fbb03ac"
source: "medium"
---

<section name="1cde" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="af4d" id="af4d" class="graf graf--p graf-after--h3">Imagine you deploy a chatbot assistant at your company. It can answer FAQs, lookup policies, book meetings — all sorts of tasks. Early on, it seems to handle single questions well. But when employees ask follow-up questions or try multi-step tasks, things start to go awry. Users get frustrated, conversations derail, and it’s hard to pinpoint why. Traditional chatbot metrics (like rating each response for correctness) aren’t telling the whole story. Was the user’s overall goal accomplished? If not, where exactly did the bot fail? These questions are crucial — and until now, hard to answer with existing evaluation methods .</p><p name="ba02" id="ba02" class="graf graf--p graf-after--p">Our recent paper, <a href="https://arxiv.org/abs/2510.03696" data-href="https://arxiv.org/abs/2510.03696" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">“Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models</a>” tackles this very problem. In this post, we’ll break down the practical impact of our work in an engaging, (hopefully) non-academic way. Whether you’re a developer, product manager, or AI enthusiast, this new evaluation framework offers a fresh perspective on measuring chatbot success, one that could transform how we improve AI agents.</p><h3 name="d3eb" id="d3eb" class="graf graf--h3 graf-after--p">Why Traditional Chatbot Evaluation Falls Short</h3><p name="9cd2" id="9cd2" class="graf graf--p graf-after--h3">Today’s chatbot evaluations mostly operate at the turn level — checking if each question/answer pair is relevant or correct . Those metrics (like BLEU scores, response relevance ratings, etc.) are useful, but they miss the forest for the trees. Users care about accomplishing their end goal, not just getting one good reply. A conversation might have several back-and-forth turns; even if most turns are fine, a single failure can ruin the user’s experience. Unfortunately, most existing methods lack a unified view to assess if the user’s overall goal was achieved or to diagnose where a multi-turn exchange went wrong .</p><p name="d4a0" id="d4a0" class="graf graf--p graf-after--p">And multi-turn exchanges do go wrong more often. In our analysis of an enterprise assistant (called AIDA), about 39% of chats had multiple turns, and those multi-turn sessions caused a disproportionate share of user frustration. In fact, users left negative feedback (e.g. saying the bot was unhelpful) three times more often in multi-turn dialogues compared to one-and-done questions (2.65% vs 0.9% negative feedback rate) . Clearly, longer conversations have higher risk of failure. But without a better evaluation approach, it’s hard to measure or fix that.</p><p name="8e1b" id="8e1b" class="graf graf--p graf-after--p">So, what’s the solution? We need to “mind the goal” to evaluate chatbots based on goals achieved rather than just turns completed. Our paper introduces a framework to do exactly that, and does so in a data-efficient way (meaning we don’t have to dump an army of human annotators on the problem). Let’s break down the key ideas.</p><h3 name="88c1" id="88c1" class="graf graf--h3 graf-after--p">Our Approach at a Glance: Goals, Success, and Root Causes</h3><ol class="postList"><li name="6a94" id="6a94" class="graf graf--li graf-after--h3"><strong class="markup--strong markup--li-strong">Goal-Oriented Evaluation</strong>: We propose evaluating conversations in terms of user goals. A goal is essentially the user’s underlying task or information need — for example, “find out the office decor policy” or “apply for medical leave”. In a single chat session, a user might pursue multiple distinct goals (ask about a policy, then switch to an unrelated question, etc.). Our framework automatically segments each conversation into these goal-oriented chunks . Each goal is like a mini-dialog within the dialog, comprising one or more turns focused on that task.</li><li name="1c15" id="1c15" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Goal Success Rate (GSR)</strong>: For each goal segment, we determine if the chatbot successfully fulfilled that goal or not. We define a strict criterion: a goal is only successful if every turn in that segment was handled correctly . If any turn in the goal was a failure (incorrect answer, irrelevant response, misunderstanding, etc.), then the entire goal is marked as failed. This might sound harsh, but it mirrors user experience — even if the bot eventually gets the answer on a second try, the user still encountered a failure along the way. By this measure, the Goal Success Rate (GSR) is the percentage of user goals that were fully satisfied without any hiccups . GSR gives us a holistic success metric for multi-turn conversations that traditional turn-level accuracy would overlook.</li><li name="55ca" id="55ca" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Root Cause of Failure (RCOF) Taxonomy</strong>: Knowing a goal failed is helpful, but actionable insights come from knowing why it failed. Our framework introduces a taxonomy of seven error categories — we call them Root Causes of Failure (RCOF) — that cover common breakdowns in chatbot performance . Each failed goal (or rather, the first turn in that goal that failed) is tagged with one of these root causes. The categories include things like Language Understanding failures (the bot misunderstood the question), Retrieval failures (the bot couldn’t fetch the needed info), Incorrect Retrieval (it fetched something, but the wrong thing), Refusal (the bot refused to answer, maybe due to policy), System Error (technical glitch), Incorrect Routing (in a multi-agent system, the query went to the wrong expert agent), or Out-of-Domain queries (user asked something the system isn’t meant to handle) . By categorizing failures, developers can zero in on the top failure modes. For example, if a large chunk of failures are due to retrieval issues, you know to improve your knowledge base integration. Our paper provides a detailed defect taxonomy and shows how identifying the dominant failure reasons helps prioritize fixes .</li></ol><blockquote name="2ca6" id="2ca6" class="graf graf--blockquote graf-after--li">Taken together, GSR and RCOF offer a powerful lens: “Did the chatbot meet the user’s goal, and if not, what went wrong?” This is a big step up from just “Did the last answer make sense?”. It gives a more user-centric evaluation.</blockquote><p name="4923" id="4923" class="graf graf--p graf-after--blockquote"><strong class="markup--strong markup--p-strong">Key Highlights of Our Framework</strong>: (In case you want the TL;DR; here are the main points you should remember.)</p><ul class="postList"><li name="a815" id="a815" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Goal Success Rate (GSR)</strong>: A new metric focusing on end-to-end success of user goals (not just turn-by-turn performance). It holds the bot to a high standard — all turns must be correct for a goal to count as successful . This ensures high fidelity to real user satisfaction (no partial credit for eventually getting it right).</li><li name="68f5" id="68f5" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Root Cause Analysis</strong>: A taxonomy of failure reasons (RCOF) that pinpoints why a goal failed — from comprehension errors to retrieval misses . This makes evaluations explainable and actionable, so teams can address the root issues rather than guessing in the dark.</li><li name="acbf" id="acbf" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Automated “Teacher Model” Evaluation</strong>: We leverage large language models (LLMs) as “teacher” evaluators to label conversations, using chain-of-thought reasoning for transparency . Multiple LLMs judge each conversation (like an ensemble of AI reviewers) and we only call in humans if the AI judges disagree strongly . This setup means we get high-quality labels and explanations with minimal human effort — a data-efficient approach to evaluation.</li><li name="f0cb" id="f0cb" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Continuous Improvement Loop</strong>: By regularly measuring GSR and analyzing failures, we can drive a continuous improvement cycle for the chatbot. In our case study, this feedback loop helped increase the chatbot’s goal success from ~63% to ~79% over six months (more on this later!). The evaluation isn’t just a score — it’s a diagnostic tool guiding the product roadmap (e.g. which new capabilities to develop next).</li></ul><p name="ed7c" id="ed7c" class="graf graf--p graf-after--li">Next, let’s dive a bit deeper into how we implemented this evaluation system , especially the cool parts involving LLMs acting as evaluators (so-called CIM: Conversational Intelligence Model in the paper) — and then see the real-world impact it had.</p><h3 name="226c" id="226c" class="graf graf--h3 graf-after--p">Let AI Judge AI: Teacher Models for Scalable Evaluation</h3><p name="d38a" id="d38a" class="graf graf--p graf-after--h3">One of the challenges in any evaluation framework is: who labels the data? If you’re going to assess thousands of chatbot conversations for goal success and failure reasons, you might think you need an army of human annotators reading chats and applying those criteria. That would be expensive, slow, and not scalable (and let’s face it, pretty boring for the humans). We took a different approach: use Large Language Models themselves as the evaluators, in a carefully designed pipeline with human oversight only when needed.</p><figure name="036a" id="036a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lJ_GiH4uaHTcw0QigyN9Zw.png" data-width="1536" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*lJ_GiH4uaHTcw0QigyN9Zw.png"><figcaption class="imageCaption">Figure 1: Our goal-oriented evaluation pipeline. We feed conversation logs (dialog dataset) into multiple expert LLMs (FM1, FM2, FM3), each prompted to analyze the conversation with step-by-step reasoning (the &lt;think&gt; chain-of-thought). Each model decides the goal segmentation, whether each goal succeeded, or if not, the failure category. Their judgments are combined via majority vote. If at least two models agree, that becomes the label. Only if all three models disagree on an outcome do we flag it for a human-in-the-loop review, where an expert (following predefined guidelines, a.k.a SOPs) resolves the ambiguity . This way, the heavy lifting is done by AI, with humans handling just the tricky edge cases. Over time, this human feedback is fed back to refine the LLM prompts and guidelines (a feedback loop), continually improving the system’s consistency. We even train a distilled student model on the LLM-generated labels to get a lightweight model that can evaluate new conversations quickly (useful for real-time/offline monitoring) .</figcaption></figure><p name="ce5a" id="ce5a" class="graf graf--p graf-after--figure">Our use of “teacher” LLMs with chain-of-thought prompting is a key ingredient. We instruct the models to think step-by-step (annotating their reasoning in special &lt;think&gt;…&lt;/think&gt; tags) before giving a final judgment . This is like asking the AI to show its work. It results in interpretable rationales for each goal, the model explains why it thinks the goal succeeded or failed and, if failed, which error category applies. These rationales serve two purposes: (1) they make the evaluation explainable (so we can read why a conversation was marked as failure), and (2) they act as justifications that help ensure the model follows the criteria correctly (like an internal checklist). In essence, the AI judges are not just saying “fail” or “pass” they’re providing a mini-report for each dialog.</p><p name="54cd" id="54cd" class="graf graf--p graf-after--p">By using 3 different foundation models (in our experiments, we used variants of Claude and GPT-4, among others ), we also get some ensemble wisdom. If one model is off-base on a particular case, the others usually outvote it. We found that in the vast majority of cases, the AI judges do converge on a consensus. Only a small fraction (~5–10%) of goals ended up with all three models disagreeing, which we then sent to humans to review (with clear guidelines to ensure consistency). This drastically reduces the human labeling effort, making the approach very data-efficient — no large human-labeled training set needed upfront. Instead, we label on the fly with AI and minimal human oversight.</p><p name="efac" id="efac" class="graf graf--p graf-after--p">The outcome is a labeled dataset of chatbot goals: each goal is marked success/failure and tagged with a root cause if failed. We can use these labels to calculate overall GSR scores, breakdown of failure types, and track trends over time. And because the evaluation is largely automated, we can do this continuously (e.g. every week or month) to keep tabs on the chatbot’s health.</p><p name="c469" id="c469" class="graf graf--p graf-after--p">Before we discuss results, let’s visualize how goal-oriented evaluation actually catches issues that turn-level evaluation might miss.</p><p name="201f" id="201f" class="graf graf--p graf-after--figure">The above figure is a great illustration: if you only looked at individual turns, you might think “hey, most of the bot’s answers were okay.” But from the user’s goal perspective, two out of three tasks failed here, something you’d only catch with the goal-centric lens. By pinpointing the exact turn and reason where each goal failed (reasoning error, misunderstanding, etc.), the framework provides actionable feedback to developers. For instance, seeing several “Language Understanding (E1)” errors might prompt you to improve the NLP module or add training data for anaphora resolution. A spike in “Retrieval Failures (E4)” might signal issues with the search tool or knowledge base coverage, and so on.</p><h3 name="ee45" id="ee45" class="graf graf--h3 graf-after--p">Results: Better Insights, Better Chatbots</h3><p name="5b99" id="5b99" class="graf graf--p graf-after--h3">So, does this approach actually help make chatbots better? In our case study with the AIDA enterprise assistant, the answer is yes. By applying our evaluation, we gained insights that directly guided improvements and we can quantify the progress.</p><p name="d6c2" id="d6c2" class="graf graf--p graf-after--p">We evaluated around 10,000 real chat sessions from AIDA using our framework . Overall, the chatbot’s Goal Success Rate was about 78%, meaning roughly 78% of user goals were eventually fulfilled without any error turns . But importantly, we discovered that multi-turn goals were much less successful: GSR for dialogs spanning 2 or more turns was only 66% . In contrast, very short one-turn queries had higher success. This gap told us that complexity and follow-ups were a pain point. The RCOF analysis further revealed the top failure modes in those multi-turn dialogs: the biggest chunk were Retrieval Failures (~39%), followed by Language Understanding errors (~27%), and Incorrect Retrievals (~16%) . In plain terms, the bot often either couldn’t find the needed info at all, or it misunderstood some aspect of the query, or it fetched the wrong info. Smaller fractions were things like system errors or refusals.</p><p name="b9d2" id="b9d2" class="graf graf--p graf-after--p">Armed with this knowledge, the development team could prioritize fixes. And indeed, over the next few months, they rolled out targeted improvements: better integration with internal knowledge sources (to reduce retrieval failures), improved intent parsing and disambiguation (to tackle understanding errors), smarter routing of queries to the right expert agent, and so on . We continuously measured GSR as these updates went out. The result: a steady climb in performance. Overall GSR rose from ~64% in February to ~78% by April (about a 14-point jump), with multi-turn GSR specifically rising by 12 percentage points . By six months into deployment, GSR was hovering around 79% (up from 63% at launch) . This is a significant improvement in success rate, directly validated by our evaluation framework — we could confidently say more user goals were being satisfied, and we knew exactly in which areas. Even better, these gains weren’t achieved by simply tweaking superficial metrics or adding bandaid fixes. The paper notes that the growth “was not driven by prompt tuning or fallback rules, but rather by launching new capabilities” like improved retrieval, better routing, upgraded LLM reasoning, etc. . In other words, the framework guided real, substantive improvements in the system’s capabilities, and our metrics proved their worth by reflecting those improvements.</p><p name="79a9" id="79a9" class="graf graf--p graf-after--p">We also assessed how reliable our LLM teacher models were compared to human annotations. In cases where we had human-labeled ground truth, the AI judges agreed with humans to a high degree (details are in the paper’s Appendix). The takeaway is that our automated evaluation is not only efficient but also accurate and trustworthy enough to drive decisions.</p><h3 name="d458" id="d458" class="graf graf--h3 graf-after--p">Why This Matters (Practical Impact)</h3><p name="1036" id="1036" class="graf graf--p graf-after--h3">For practitioners and leaders implementing AI agents, evaluation is everything. You can’t improve what you don’t measure. Our goal-oriented evaluation framework provides a clear-eyed view of chatbot performance that aligns with user success. By focusing on goals, it captures aspects of quality (like multi-step reasoning, context handling, error recovery) that turn-level checks might miss. It also surfaces systemic failure patterns (through RCOF analytics) that help prioritize where to invest engineering effort.</p><p name="2f12" id="2f12" class="graf graf--p graf-after--p">A big practical win is the data efficiency of our approach. Many organizations hesitate to deploy complex evaluation schemes because of the cost and time of annotation. But here we showed you can leverage AI to evaluate AI. Large models can judge conversations pretty well, especially when given good prompts and used in an ensemble. This drastically reduces the need for human evaluators, freeing your team to focus on building features rather than labeling data. It’s like having a scalable QA team working 24/7, with humans only handling the rare corner cases.</p><p name="9607" id="9607" class="graf graf--p graf-after--p">Moreover, the framework’s explainable output (rationales and standardized error tags) bridges the gap between raw metrics and actionable insights. Instead of a black-box score, you get a autopsy for each failed interaction :“this user’s goal failed because the bot misunderstood the location in the request and pulled the wrong info” which is gold for troubleshooting. It helps engineers, product managers, even executives understand what’s holding the AI back. This can inform not just bug fixes but also strategic decisions (like maybe the bot needs a new capability, or maybe certain requests should be handed off to a human agent).</p><p name="443f" id="443f" class="graf graf--p graf-after--p">Finally, our approach is general. While we applied it to a specific multi-agent enterprise chatbot, the concept of goal-level evaluation and root cause analysis can be used for any conversational AI system, including customer support bots, virtual assistants, or even open-domain chatbots that have defined user tasks. As AI systems become more agentic, able to perform multi-step reasoning, tool use, and interact with other agents -evaluating them will only get more challenging. We believe Mind the Goal offers a way to keep these AI agents accountable to the end-user’s objectives, and to do so in a cost-effective, interpretable manner.</p><p name="f83a" id="f83a" class="graf graf--p graf-after--p">In summary, we’re excited about this work because it shifts the focus of chatbot evaluation to what truly matters: user outcomes. It provides a concrete methodology to measure and improve those outcomes continuously. If you’re interested in digging into the technical details, examples, and additional analyses, we encourage you to check out our paper and join the discussion.</p><p name="2d8d" id="2d8d" class="graf graf--p graf-after--p">We’d love to hear feedback or questions from the community. Feel free to discuss or ask questions on our Hugging Face page for the paper, or reach out to us directly. We hope this framework can spark further research and help others build more reliable conversational AI. Together, let’s move the needle on chatbot quality in a way that users will feel, by ensuring their goals are met, one conversation at a time!</p><p name="1031" id="1031" class="graf graf--p graf-after--p">Citation: If you find our work useful, you can refer to it as:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="cpp" name="af6e" id="af6e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">@article{piskala2025mind,<br />  title={{Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents <span class="hljs-keyword">and</span> Chatbots <span class="hljs-keyword">using</span> Teacher Models}},<br />  author={Piskala, Deepak Babu <span class="hljs-keyword">and</span> Chen, Sharlene <span class="hljs-keyword">and</span> Patel, Udita <span class="hljs-keyword">and</span> Kalra, Parul <span class="hljs-keyword">and</span> Castrillo, Rafael},<br />  journal={arXiv preprint arXiv:<span class="hljs-number">2510.03696</span>},<br />  year={<span class="hljs-number">2025</span>}<br />}</span></pre><p name="07f3" id="07f3" class="graf graf--p graf-after--pre graf--trailing">Feel free to share this with anyone interested in conversational AI evaluation. Let’s all start minding the goal when we build and judge our AI systems!</p></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/mind-the-goal-a-data-efficient-framework-to-evaluate-conversational-and-agenticai-and-why-it-bc209fbb03ac">https://medium.com/@prdeepak.babu/mind-the-goal-a-data-efficient-framework-to-evaluate-conversational-and-agenticai-and-why-it-bc209fbb03ac</a></p>
