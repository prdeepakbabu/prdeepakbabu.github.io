---
title: "Scalable Oversight in AI: Beyond Human Supervision"
date: "2024-05-20"
coverImage: "https://cdn-images-1.medium.com/max/1024/1*S44j_t24sZsIgcMJMh4xLg.png"
canonical: "https://medium.com/@prdeepak.babu/scalable-oversight-in-ai-beyond-human-supervision-d258b50dbf62"
mediumId: "d258b50dbf62"
source: "medium"
---

<section name="074e" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="b1e3" id="b1e3" class="graf graf--p graf-after--figure">In recent years, the field of artificial intelligence has experienced unprecedented growth. Two driving forces underpin this advancement: the availability of colossal datasets, exemplified by the Llama 3 model trained on 15 trillion tokens, and the emergence of models with trillions of parameters, such as GPT-4. Historically, human supervision has been pivotal in training and evaluating AI models. However, as these models achieve superhuman accuracies in specific domains, we confront a fundamental challenge: How do we evaluate and improve systems that outperform humans? Furthermore, how do we train models that exceed human accuracy?</p><h3 name="e6f0" id="e6f0" class="graf graf--h3 graf-after--p">The Inflection Point: A Personal Account — ASR</h3><p name="dfb0" id="dfb0" class="graf graf--p graf-after--h3">During my tenure from 2019 to 2024 as a senior scientist working on speech recognition for a leading voice assistant — Alexa, I witnessed remarkable advancements. The enhancements in <a href="https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac" data-href="https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac" class="markup--anchor markup--p-anchor" target="_blank">ASR technology</a> can be summarized into three key developments: (i) the gradual replacement of traditional acoustic models with deep neural networks (DNNs), (ii) the shift from n-gram to subword neural language models, and (iii) the integration of acoustic and language models into a unified end-to-end model focused on minimizing mWER (minimum WER). Word error rates (WER) plummeted from around 15% to below 3% on testsets of interest during this period. However, sometime around 2022, we encountered an unexpected inflection point. Despite implementing significant upgrades to our speech models, our evaluation metrics began to exhibit regression. Metrics tracked during our Weekly Business Reviews (WBRs) and Monthly Business Reviews (MBRs) became erratic and inconsistent.</p><p name="94d3" id="94d3" class="graf graf--p graf-after--p">A particular incident highlighted this challenge. After rolling out a major update to our ASR system, our internal testing showed inexplicable metric fluctuations. We meticulously reviewed the updates and found no immediate issues. We started to look at a disagreements between machine and human transcriptions and closely analyzing the source of error. It was then that we realized our models had achieved such high accuracy that human transcriptions, traditionally our gold standard, were no longer reliable. The WER, based on human transcription, was comparable (at times exceeding) to the best ASR model in production. This revelation underscored a critical dilemma: How do we further enhance and evaluate models that have surpassed human transcription capabilities? Our models had not just become noise-robust but also built deep understanding of entities that even expert humans would struggle.</p><p name="de61" id="de61" class="graf graf--p graf-after--p">I am sure there are similar stories emerging from narrow tasks in different domains, where such stories are unfolding. Each problem is unique and requires innovation. In the later section, we highlight a couple of potential directions to explore for continuously improving and evaluating super-human AI models.</p><h3 name="c21f" id="c21f" class="graf graf--h3 graf-after--p">What is scalable oversight ?</h3><p name="9c51" id="9c51" class="graf graf--p graf-after--h3">Scalable oversight refers to the systematic approach and methodologies required to monitor, evaluate, and enhance AI systems as they grow in complexity and capability, particularly when they exceed human performance. Not surprisingly many foundation model companies are already thinking deeply on this topic as we see <a href="https://medium.com/@prdeepak.babu/emergent-abilities-in-llm-unpredictable-abilities-in-large-language-models-72fc54059b41" data-href="https://medium.com/@prdeepak.babu/emergent-abilities-in-llm-unpredictable-abilities-in-large-language-models-72fc54059b41" class="markup--anchor markup--p-anchor" target="_blank">emergent abilities</a> and capabilities arising from scaling LLMs based on size and data.</p><p name="b007" id="b007" class="graf graf--p graf-after--p">Anthropic discusses some of its strategies for scalable oversight in this <a href="https://arxiv.org/pdf/2211.03540" data-href="https://arxiv.org/pdf/2211.03540" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">paper</a> and proposed a technique called constitutional AI (<a href="https://arxiv.org/abs/2309.00267" data-href="https://arxiv.org/abs/2309.00267" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RLAIF</a> — RL from AI feedback) which uses another stronger model to score a student model.</p><figure name="819b" id="819b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Fx-uxFc6TFk9uFdNdJvG1Q.png" data-width="1010" data-height="468" src="https://cdn-images-1.medium.com/max/800/1*Fx-uxFc6TFk9uFdNdJvG1Q.png"><figcaption class="imageCaption">Anthropic’s Scalable Oversight — RLAIF</figcaption></figure><blockquote name="6dd9" id="6dd9" class="graf graf--blockquote graf--hasDropCapModel graf-after--figure">OpenAI introduced a new research direction called “<a href="https://arxiv.org/abs/2312.09390" data-href="https://arxiv.org/abs/2312.09390" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">superalignment</a>” back in Dec 2023, focusing on the challenge of aligning future superhuman AI systems with human supervision. The core issue is that humans, as “weak supervisors,” need to control AI systems much smarter than themselves.</blockquote><p name="64c4" id="64c4" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--blockquote"><span class="graf-dropCap">T</span>he research explores an analogy where small models (e.g., GPT-2) supervise larger models (e.g., GPT-4), achieving significant performance improvements. Initial results show that GPT-2-level models can help elicit GPT-4’s capabilities, approaching GPT-3.5-level performance. This method, although preliminary, demonstrates the potential to improve weak-to-strong generalization, highlighting the limitations of current methods like reinforcement learning from human feedback (RLHF) for superhuman AI alignment. The Superalignment team aims to ensure that even the most advanced AI systems remain safe and beneficial, making iterative empirical progress toward this goal.</p><h3 name="dd19" id="dd19" class="graf graf--h3 graf-after--p">Strategic Approaches for Scalable Oversight</h3><p name="ffb6" id="ffb6" class="graf graf--p graf-after--h3">Scalable oversight is critical in managing and improving AI systems, especially as they reach superhuman capabilities. Below, I detail several strategies designed to enhance and evaluate superhuman models:</p><ol class="postList"><li name="6b56" id="6b56" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Model-Based Evaluation (Teacher-Student Paradigm)</strong>: Utilizing the teacher-student model paradigm involves a student model learning from a more advanced teacher model. This technique helps bridge knowledge gaps between the models. Typically, the teacher model is a much larger neural network or possesses access to additional signals that the student model does not, such as proprietary datasets or more extensive training regimes. This set-up allows the teacher model to guide and correct the student model, providing a framework for continuous improvement and evaluation. For example, a resource-unconstrained version of an AI could act as a teacher, offering insights and corrections to optimize the student model’s performance and reliability. ex: RLAIF from Anthropic.</li><li name="9cbd" id="9cbd" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Large-Scale Human Feedback</strong>: Despite AI models surpassing human performance in various tasks, human insights remain crucial, particularly for their nuanced understanding and contextual awareness. To harness this, scalable oversight mechanisms can incorporate large-scale human feedback, aggregating inputs from a diverse group of users. This feedback can be explicit, such as direct user reviews or annotations, or implicit, derived from engagement metrics like click-through rates, browsing patterns, and user interactions. Such a diverse feedback loop helps address complex scenarios that require deep contextual or cultural understanding, ensuring the AI’s decisions are well-rounded and appropriately calibrated.</li><li name="aad3" id="aad3" class="graf graf--li graf-after--li"><a href="https://medium.com/@prdeepak.babu/harnessing-the-power-of-synthetic-data-in-the-era-of-large-language-models-llms-and-generative-ai-f67763256d44" data-href="https://medium.com/@prdeepak.babu/harnessing-the-power-of-synthetic-data-in-the-era-of-large-language-models-llms-and-generative-ai-f67763256d44" class="markup--anchor markup--li-anchor" target="_blank"><strong class="markup--strong markup--li-strong">Synthetic Data Utilization</strong></a>: Generating <a href="https://medium.com/@prdeepak.babu/harnessing-the-power-of-synthetic-data-in-the-era-of-large-language-models-llms-and-generative-ai-f67763256d44" data-href="https://medium.com/@prdeepak.babu/harnessing-the-power-of-synthetic-data-in-the-era-of-large-language-models-llms-and-generative-ai-f67763256d44" class="markup--anchor markup--li-anchor" target="_blank">synthetic data</a> offers a way to supplement training datasets with a broader spectrum of scenarios, including rare or edge cases not typically found in real-world data. This approach not only aids in training more robust and adaptable models but also allows for continuous testing and refinement under controlled conditions. For instance, simulating environments for AI to engage in self-play or scripted interactions can yield valuable data streams for training and evaluation. This method helps in identifying and mitigating potential biases or weaknesses in the model, ensuring it performs well across a wide range of real and hypothetical situations.</li></ol><p name="d0c0" id="d0c0" class="graf graf--p graf-after--li">Consider the example of a self-driving car, which represents a specific application of narrow AI. Vision models within these systems have demonstrated human-like capabilities and continue to improve, making increasingly better decisions under complex driving conditions. These systems must evaluate multiple states and project several steps ahead to optimally adjust steering, acceleration, and deceleration. In such scenarios, scalable oversight mechanisms are not just beneficial — they are essential. These mechanisms ensure that every action taken by full self-driving (FSD) models can be justified and validated.</p><p name="0cb5" id="0cb5" class="graf graf--p graf-after--p graf--trailing">Let us commit to a future where AI is not only powerful and pervasive but also principled and protective. By fostering a collaborative environment for developing scalable oversight, we can ensure AI systems are aligned with human values and capable of contributing positively to our collective future.</p></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/scalable-oversight-in-ai-beyond-human-supervision-d258b50dbf62">https://medium.com/@prdeepak.babu/scalable-oversight-in-ai-beyond-human-supervision-d258b50dbf62</a></p>
