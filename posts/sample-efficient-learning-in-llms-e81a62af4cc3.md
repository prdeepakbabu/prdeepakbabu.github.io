---
title: "Sample Efficient Learning in LLMs"
date: "2024-04-09"
coverImage: "https://cdn-images-1.medium.com/max/1024/0*h4Pi7sd7FfBmJFo9"
canonical: "https://medium.com/@prdeepak.babu/sample-efficient-learning-in-llms-e81a62af4cc3"
mediumId: "e81a62af4cc3"
source: "medium"
---

<section name="fa36" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="6192" id="6192" class="graf graf--h3 graf-after--figure">Sample Efficiency (Data)</h3><p name="6b5c" id="6b5c" class="graf graf--p graf-after--h3">In the realm of machine learning, “sample efficiency” is a term that defines a model’s ability to learn effectively from a limited number of examples. High sample efficiency implies that a model can quickly assimilate knowledge, needing fewer data points to make accurate predictions or understand complex patterns. This idea becomes especially striking when comparing the learning abilities of humans with those of large language models (LLMs).<br>Humans are remarkably sample-efficient learners. From a very young age, individuals can grasp new concepts, languages, and skills from relatively few examples. This efficiency stems from our innate ability to utilize prior knowledge, contextual cues, and a sophisticated understanding of the world to infer new information from sparse data.</p><figure name="c41b" id="c41b" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/Le87ypCth7c?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="4263" id="4263" class="graf graf--p graf-after--figure">Conversely, LLMs, such as those powering sophisticated chatbots and AI assistants, are trained on vast corpora comprising trillions of tokens. These models rely on extensive exposure to diverse datasets to “understand” language patterns, nuances, and the intricacies of human dialogue. Despite their impressive capabilities, these models lack the inherent intuition and real-world understanding that humans possess, necessitating their extensive training on large datasets to achieve a semblance of human-like understanding and fluency.</p><figure name="5f8c" id="5f8c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*9PZJ0tSzlXpSVbfn" data-width="757" data-height="702" src="https://cdn-images-1.medium.com/max/800/0*9PZJ0tSzlXpSVbfn"><figcaption class="imageCaption">source : <a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613%2823%2900203-6" data-href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(23)00203-6" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(23)00203-6</a>. LLMs, such as GPT-3 and Chinchilla, are trained on orders of magnitude more language data (up to 5 x ¹⁰¹¹ tokens for GPT-3 and ¹⁰¹² for Chinchilla) than what a human child is exposed to from birth through adulthood. A child’s linguistic input, even in the most language-rich environments, is estimated at a maximum of 4 x ¹⁰⁸ words by the age of 20, highlighting a stark gap in language exposure between LLMs and humans.</figcaption></figure><p name="7501" id="7501" class="graf graf--p graf-after--figure">Chart above illustrates the vast disparity in learning seen in humans vs. LLMs. LLMs, such as GPT-3 and Chinchilla, are trained on orders of magnitude more language data (up to 5 x ¹⁰¹¹ tokens for GPT-3 and ¹⁰¹² for Chinchilla) than what a human child is exposed to from birth through adulthood. A child’s linguistic input, even in the most language-rich environments, is estimated at a maximum of 4 x ¹⁰⁸ words by the age of 20, highlighting a stark gap in language exposure between LLMs and humans.</p><p name="5486" id="5486" class="graf graf--p graf-after--p">LLMs, such as GPT-3 and Chinchilla, are trained on orders of magnitude more language data (up to 5 x 1⁰¹¹ tokens for GPT-3 and 1⁰¹² for Chinchilla) than what a human child is exposed to from birth through adulthood. A child’s linguistic input, even in the most language-rich environments, is estimated at a maximum of 4 x 1⁰⁸ words by the age of 20, highlighting a stark gap in language exposure between LLMs and humans.</p><blockquote name="acd8" id="acd8" class="graf graf--blockquote graf-after--p">The discrepancy in sample efficiency between humans and LLMs highlights a significant challenge and opportunity within the AI field. Enhancing the sample efficiency of LLMs could lead to more agile, adaptable, and efficient AI systems that require less data to learn, reducing computational and energy costs. This has implications to learning and doing inference on the edge (on-device)</blockquote><p name="73aa" id="73aa" class="graf graf--p graf-after--blockquote">Furthermore, it would allow for more robust learning in data-scarce environments, opening new avenues for AI applications in fields where extensive data collection is impractical or impossible.</p><h3 name="2131" id="2131" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Energy efficiency (Compute)</strong></h3><p name="df8e" id="df8e" class="graf graf--p graf-after--h3">Comparing the energy efficiency of humans and Large Language Models (LLMs) like those running on GPUs reveals a stark contrast in energy consumption patterns. <br><strong class="markup--strong markup--p-strong">Training</strong> : LLMs, especially during their training phase, are incredibly energy-intensive. For instance, training a model like ChatGPT can consume as much energy as over 1,000 U.S. households use in a year. Daily operations to support billions of queries can equate to the daily energy consumption of about 33,000 U.S. households​ (<a href="https://www.washington.edu/news/2023/07/27/how-much-energy-does-chatgpt-use/" data-href="https://www.washington.edu/news/2023/07/27/how-much-energy-does-chatgpt-use/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">UW Homepage</a>)​. On the other hand, the human brain is an exemplar of energy efficiency. Despite its complex functionalities, the brain operates on roughly the same energy required to power a light bulb, about 20 watts. This efficiency stems from the brain’s use of sparse representations, structured data processing, continual learning capabilities, and the inherently optimized biological hardware it runs on​.<br><strong class="markup--strong markup--p-strong">Inference</strong> : As per this <a href="https://mbzuai.ac.ae/news/emulating-the-energy-efficiency-of-the-brain/" data-href="https://mbzuai.ac.ae/news/emulating-the-energy-efficiency-of-the-brain/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">article</a>, One query to ChatGPT uses the equivalent of 3.96 watts, or a third of the battery capacity of an iPhone 13 Pro. That is a large consumption of energy for one question. In a whole day, an adult human brain consumes slightly more than a third of a watt of energy, or approximately 8% the energy of one question posed to a large-language model</p><blockquote name="dd2a" id="dd2a" class="graf graf--blockquote graf-after--p">One query to ChatGPT uses the equivalent of 3.96 watts, or a third of the battery capacity of an iPhone 13 Pro. An adult human brain consumes just 8% of this energy.</blockquote><p name="5d34" id="5d34" class="graf graf--p graf-after--blockquote">The comparison underscores the need for AI research to draw inspiration from neuroscience to develop more energy-efficient computing models and hardware. Implementing strategies such as sparsity, structured learning, and continual learning in AI systems could significantly reduce their carbon footprint, moving closer to the brain’s model of efficiency.</p><h3 name="4a86" id="4a86" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Improving LLM Efficiency</strong></h3><p name="9e56" id="9e56" class="graf graf--p graf-after--h3">To tackle this issue, we could explore solutions inspired by biological learning processes, including strategies like transfer learning, sparsity, meta-learning and incremental learning. By adopting such bio-inspired methodologies, we edge closer to reducing the discrepancy in sample efficiency between human cognition and machine algorithms.</p><ol class="postList"><li name="c36c" id="c36c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Sparsity</strong></li></ol><figure name="caa1" id="caa1" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*LGJ-tUiSff4CfV3zvWQ5-Q.png" data-width="135" data-height="119" src="https://cdn-images-1.medium.com/max/800/1*LGJ-tUiSff4CfV3zvWQ5-Q.png"></figure><p name="a8f4" id="a8f4" class="graf graf--p graf-after--figure">In the intricate play of neurons that defines human cognition, the principle of sparsity plays a lead role. Our brains, with their trillions of connections, choose to engage only a select few at any given moment, weaving thoughts and actions with remarkable efficiency. Sparse Mixture of Experts (Sparse MoE) models are indeed a promising way to improve the efficiency of AI models during inference. These models utilize a conditional computation strategy where only specific parts of the network, known as “experts,” are activated based on the input. This approach significantly reduces the computational requirements compared to traditional dense models, where the entire network is active for every input.</p><p name="8611" id="8611" class="graf graf--p graf-after--p">While OpenAI hasn’t revealed the GPT4 architecture publicly, its widely believed to be a 1.76 trillion parameter model with 16 experts and each expert being a 111B parameter NN. At inference time, no more than 2 experts are active which makes it practically a 222B parameter model with virtually 1.76 trillion parameters accessible to its experts. In open source world, Mistral released open MoE model <code class="markup--code markup--p-code">Mixtral 8 x 7B</code> indicating 8 experts each with 7B parameter model leading to a virtual 56B model but practically only 14B parameters of its model is active at any point since it uses 2 experts for routing. <br><strong class="markup--strong markup--p-strong">LoRA (PEFT)</strong> : Low-Rank Adaptation (LoRA) is a technique used to adapt pre-trained models, such as Large Language Models (LLMs), with a focus on maintaining computational efficiency and leveraging the power of sparsity. LoRA introduces trainable parameters that act as adaptations to the existing weights of a pre-trained model, allowing for fine-tuning on specific tasks without altering the original model weights directly.</p><p name="8789" id="8789" class="graf graf--p graf-after--p">2. <a href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f?source=friends_link&amp;sk=33f32534b99589fb07f4d11ae5b8892b" data-href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f?source=friends_link&amp;sk=33f32534b99589fb07f4d11ae5b8892b" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">Multimodality</strong></a></p><figure name="892b" id="892b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CvD7ltBcbhUpSB0GucQ6Qg.png" data-width="210" data-height="122" src="https://cdn-images-1.medium.com/max/800/1*CvD7ltBcbhUpSB0GucQ6Qg.png"></figure><p name="5128" id="5128" class="graf graf--p graf-after--figure">Multimodality in AI refers to the integration of various types of data (e.g., text, images, audio) to create more comprehensive models. This approach can indeed enhance sample efficiency by leveraging the rich, complementary information contained in different data modalities. For instance, a recent survey highlighted the importance of mixtures of experts (MoE) and multimodal learning in pushing the boundaries of generative AI. Such systems, like Google’s Gemini, are redefining benchmarks by processing and understanding multiple modalities, leading to more robust and versatile models. The ability of these systems to handle a broad range of inputs without significantly increasing model size underscores the potential of multimodality in achieving more efficient AI<br>​Text-only models lack multidimensional perception like vision and audio and hence it becomes infeasible to project these in text domain. For example, you can imagine the number of tokens needed to describe a scene in text domain, it can quickly become exponential as there could be multiple objects to emphasize. Instead<a href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8?source=friends_link&amp;sk=7ed9c6694d590fac0a8deb401ee13f84" data-href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8?source=friends_link&amp;sk=7ed9c6694d590fac0a8deb401ee13f84" class="markup--anchor markup--p-anchor" target="_blank"> multimodal models</a> (LLM or LMM) model perception mimicking humans to represent (image, audio, text) as inputs to the LMM could solve the problem of accurately being able to build world models with minimal data.</p><p name="9e0e" id="9e0e" class="graf graf--p graf-after--p">3. <strong class="markup--strong markup--p-strong">Differential Compute</strong></p><figure name="fe26" id="fe26" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nXB2p9Bm_u-UbqEkHE6h0A.png" data-width="117" data-height="170" src="https://cdn-images-1.medium.com/max/800/1*nXB2p9Bm_u-UbqEkHE6h0A.png"></figure><p name="f2fb" id="f2fb" class="graf graf--p graf-after--figure">Regarding differential compute, the idea is inspired by the human brain’s dual-process theory, where System 1 involves fast, intuitive thinking, and System 2 entails slower, more analytical reasoning. Applying this analogy to AI, we could allocate computational resources dynamically, using less compute for simpler tasks (akin to System 1 processes) and more for complex problems that require deeper analysis (similar to System 2 processes). While specific research on this concept wasn’t directly found in the recent search, it’s clear that the AI community is exploring similar ideas through adaptive and efficient model architectures. The goal is to make AI systems more flexible and resource-efficient, enabling them to tackle a wide range of tasks without uniformly high computational demands.<br>Differential compute, in the context of AI and machine learning, refers to the strategic allocation of computational resources based on the complexity or demands of a specific task or component of a model. The core idea is not to have a one-size-fits-all approach to computing resources across all tasks but to dynamically adjust the computational effort according to the requirements of each task.</p><p name="6f12" id="6f12" class="graf graf--p graf-after--p">For example, simpler tasks or parts of a model that are less crucial for performance might receive fewer computational resources, while more complex tasks or critical components might receive more. This approach is inspired by the human brain’s ability to dynamically allocate attention and processing power depending on the task at hand — a concept sometimes related to the brain’s “System 1” and “System 2” thinking processes, as popularized by Daniel Kahneman in his book “Thinking, Fast and Slow.”</p><blockquote name="3a17" id="3a17" class="graf graf--blockquote graf-after--p">In System 1 (fast, automatic, frequent, emotional, stereotypic, subconscious thinking), the brain handles tasks with minimal effort and energy. In contrast, System 2 (slow, effortful, infrequent, logical, calculating, conscious thinking) is activated for tasks that require deep thought, reasoning, and higher cognitive processes, thereby consuming more energy.</blockquote><p name="3da2" id="3da2" class="graf graf--p graf-after--blockquote">Applying a similar differential compute strategy in AI systems, especially in large-scale models like LLMs, can lead to more efficient use of computational resources, potentially reducing the time and energy required for training and inference, enhancing the model’s performance, and making AI more scalable and sustainable.<br>[1] <a href="https://arxiv.org/abs/2404.02258" data-href="https://arxiv.org/abs/2404.02258" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Mixture-of-Depths: Dynamically allocating compute in transformer-based language models</a><br>[2] <a href="https://arxiv.org/pdf/2211.17192.pdf" data-href="https://arxiv.org/pdf/2211.17192.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Fast Inference from Transformers via Speculative Decoding</a><br>Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens</p><p name="382f" id="382f" class="graf graf--p graf-after--p">4. <strong class="markup--strong markup--p-strong">Curriculum Learning</strong></p><figure name="af55" id="af55" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_SKrl1ljjd1oiSuIW8WGuQ.png" data-width="139" data-height="163" src="https://cdn-images-1.medium.com/max/800/1*_SKrl1ljjd1oiSuIW8WGuQ.png"></figure><p name="13dd" id="13dd" class="graf graf--p graf-after--figure">As humans we learn by gaining knowledge in sequential manner by gaining fundamental understanding of language and science concepts in lower grades and slowly building on this fundamental during higher grades, understanding by specializing in domains like electronics, medicine, engineering and advanced sciences with sub-domains like construction, manufacturing and computers. Similarly, we can think of pre-training LLM to also follow a curriculum i.e plan for incrementally exposing knowledge. This relates to decisions like data mixtures, domain representations, time-sensitive representations, task mixtures (QA, code and chat). This involves questions like</p><ul class="postList"><li name="5ae3" id="5ae3" class="graf graf--li graf-after--p">Should we expose recent data on day 4 of pretraining vs. day 20 of pre-training ?</li><li name="54d0" id="54d0" class="graf graf--li graf-after--li">when should you expose domain specific data pre-training ?</li><li name="8570" id="8570" class="graf graf--li graf-after--li">How should we handle duplicate information ?</li><li name="d887" id="d887" class="graf graf--li graf-after--li">Should we focus on quality of text used for pre-training ?</li><li name="11c1" id="11c1" class="graf graf--li graf-after--li">Should we pre-train based on chronological order of events and news ?</li></ul><p name="93fd" id="93fd" class="graf graf--p graf-after--li">As you can imagine there are 1000s of potential combinations to tune towards experimenting every single strategy and is practically infeasible to arrive at the optimal setting specially given how expensive it is to pre-train using special purpose GPUs.</p><p name="9020" id="9020" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--p"><span class="graf-dropCap">C</span><strong class="markup--strong markup--p-strong">urriculum learning</strong> is an efficiency-enhancing strategy for model training that organizes the introduction of training instances from simpler to more complex, based on a defined difficulty metric and a pacing function. The difficulty metric ranks training samples by complexity, using common indicators like sequence length or vocabulary rarity, and can be tailored to specific tasks with more nuanced metrics such as edit distance in paraphrase generation or linguistically-motivated features in neural machine translation. The pacing function, whether step-wise with linear or exponential curves or stage-wise as in models like Shortformer, controls the gradual increase in task complexity. Self-paced learning integrates these components, adjusting sample selection and learning parameters iteratively to encompass the entire training set progressively. While curriculum learning has proven to enhance data efficiency in LLM pretraining and finetuning, determining the optimal difficulty metrics and pacing speed remains an empirical challenge that necessitates task or model-specific exploration.</p><figure name="047b" id="047b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EF8wkw8ZUXytxPaPUFefUQ.png" data-width="901" data-height="409" src="https://cdn-images-1.medium.com/max/800/1*EF8wkw8ZUXytxPaPUFefUQ.png"><figcaption class="imageCaption">source: <a href="https://arxiv.org/html/2403.04652v1" data-href="https://arxiv.org/html/2403.04652v1" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/html/2403.04652v1</a></figcaption></figure><p name="a513" id="a513" class="graf graf--p graf-after--figure">[1] <a href="https://arxiv.org/abs/2306.11644" data-href="https://arxiv.org/abs/2306.11644" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Textbooks are all you need</a> empahsized the contribution of high-quality data impact on LLM performance and proved smaller 1.5B models can often reach the quality of 7B/14B models given high quality dataset over low quality high volume data.<br>[2] <a href="https://arxiv.org/html/2403.04652v1" data-href="https://arxiv.org/html/2403.04652v1" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Yi: Open Foundation Models by 01.AI</a> showed detailed pre-training regime and show how LLM performance boils down to simple data quality issues addressed during pre-training. The discuss data mixtures, model-based filtering, deduplication and topic sampling including task mixtures. These insights are rarely shared by research labs building LLMs and considered to be the secret sauce that is a deciding factor for its performance in leaderboards.</p><p name="7450" id="7450" class="graf graf--p graf-after--p">5. <strong class="markup--strong markup--p-strong">Model Merging</strong></p><figure name="de0d" id="de0d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*wQwiQE_ouqKipY9AasgnRA.png" data-width="132" data-height="124" src="https://cdn-images-1.medium.com/max/800/1*wQwiQE_ouqKipY9AasgnRA.png"></figure><p name="af58" id="af58" class="graf graf--p graf-after--figure">Model merging, or the process of combining multiple Large Language Models (LLMs) into a single model, is an emerging technique that could potentially enhance sample efficiency. This method amalgamates the strengths and capabilities of individual models, allowing the merged entity to exhibit the combined knowledge and functionality without necessitating additional training.</p><p name="994c" id="994c" class="graf graf--p graf-after--p">For instance, the use of Mergekit allows for the combination of models such as Mistral-7B with others, employing methods like TIES (TrIm, Elect, and Merge) and DARE (Drop And REscale). TIES involves trimming redundant parameters, resolving sign conflicts, and then averaging parameters that align in sign, while DARE involves pruning fine-tuned weights back to their base model values and rescaling weights to maintain model output expectations​ (<a href="https://huggingface.co/blog/mlabonne/merge-models" data-href="https://huggingface.co/blog/mlabonne/merge-models" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">H</a>F)​. Such approaches ensure that the merged model retains only the most effective parameters from each constituent model, potentially leading to more efficient computation and improved performance on tasks that require a broad range of knowledge.</p><p name="1565" id="1565" class="graf graf--p graf-after--p">Moreover, the concept of model merging extends to adapting models to specific tasks without extensive retraining, as detailed in PEFT documentation. This is particularly beneficial for multitask learning, where a single model is optimized to perform various tasks. Techniques like TIES and DARE facilitate this by efficiently combining LoRA adapters from different models, thereby enhancing the merged model’s capabilities without significant computational overhead​ (<a href="https://huggingface.co/docs/peft/v0.8.2/en/developer_guides/model_merging" data-href="https://huggingface.co/docs/peft/v0.8.2/en/developer_guides/model_merging" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face</a>)​.</p><p name="8ae7" id="8ae7" class="graf graf--p graf-after--p">This strategy of merging models aligns with the broader goal of achieving more versatile and efficient AI systems, capable of generalizing across a wide array of tasks with minimal additional training. It also addresses the challenge of managing the storage and computational resources required by multiple large-scale models, making it a promising direction for future research in AI efficiency and effectiveness.</p><p name="1a5a" id="1a5a" class="graf graf--p graf-after--p">[1] <a href="https://arxiv.org/abs/2403.13187" data-href="https://arxiv.org/abs/2403.13187" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Evolutionary Optimization of Model Merging Recipes</a> <br>The paper introduces a novel approach to model merging through the application of evolutionary algorithms, aiming to automate the creation of powerful foundation models. This method significantly deviates from traditional, intuition-based model merging, offering a systematic way to explore and combine a diverse range of open-source models without the need for additional training. By operating in both parameter space and data flow space, the approach enables not only the optimization of model weights but also the innovation in how data traverses through model architectures, allowing for the creation of models with new, combined capabilities. The paper showcases the success of this method with the development of a Japanese LLM with Math reasoning abilities and a culturally-aware Japanese VLM, both achieving state-of-the-art performance in their respective domains. This work not only contributes new, high-performing models to the open-source community but also establishes a new paradigm for model development, emphasizing the potential of evolutionary algorithms in discovering efficient and</p><p name="e718" id="e718" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Related Posts</strong></p><div name="9262" id="9262" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" data-href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8"><strong class="markup--strong markup--mixtapeEmbed-strong">Multimodal Mastery: The Qwen Audio Foundation Models for Advanced Audio Understanding and Reasoning</strong><br><em class="markup--em markup--mixtapeEmbed-em">As a follow-up to the last blog on large multimodal audio models (LMM), we’re here to explore an open-source large LMM…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="80499b434648be7b8354e4365573f27c" data-thumbnail-img-id="1*GZ-3hMim6LtmAiRUGeWSXw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*GZ-3hMim6LtmAiRUGeWSXw.png);"></a></div><div name="a539" id="a539" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed graf--trailing"><a href="https://medium.com/@prdeepak.babu/decoding-ais-thought-process-mechanistic-interpretability-54db5727e7e8" data-href="https://medium.com/@prdeepak.babu/decoding-ais-thought-process-mechanistic-interpretability-54db5727e7e8" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/decoding-ais-thought-process-mechanistic-interpretability-54db5727e7e8"><strong class="markup--strong markup--mixtapeEmbed-strong">Decoding AI’s Thought Process: Mechanistic Interpretability</strong><br><em class="markup--em markup--mixtapeEmbed-em">Mechanistic Interpretability is a field of study that concerns study of neural networks (more generally ML models) with…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/decoding-ais-thought-process-mechanistic-interpretability-54db5727e7e8" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="80c3fec295c1e181f417906cee826743" data-thumbnail-img-id="0*-Z4XL91lYNGItrL0" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*-Z4XL91lYNGItrL0);"></a></div></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/sample-efficient-learning-in-llms-e81a62af4cc3">https://medium.com/@prdeepak.babu/sample-efficient-learning-in-llms-e81a62af4cc3</a></p>
