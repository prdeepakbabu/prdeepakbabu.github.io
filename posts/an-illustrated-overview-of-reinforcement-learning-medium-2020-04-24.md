---
title: "An Illustrated Overview of Reinforcement Learning"
date: "2020-04-24"
coverImage: "https://cdn-images-1.medium.com/max/1024/0*sgPbLhy-zy5ASeWp.png"
canonical: "https://medium.com/@prdeepak.babu/an-illustrated-overview-of-reinforcement-learning-ccc47ae43b6"
mediumId: "ccc47ae43b6"
source: "medium"
---

<section name="3b4b" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="2143" id="2143" class="graf graf--p graf-after--h3">Reinforcement Learning (RL) is a learning paradigm different from traditional machine learning (supervised and unsupervised). The learning problem considered here mimics humans learning from interactions using trial-and-error and has historically been used in the context of planning/decision making related problems like robotics and autonomous driving. We can apply RL in cases where there is a need to make sequential decisions to optimize a metric. Let’s consider an example of news recommendation problem. Users are presented list of articles that are of potential interest to them and user decides to either click(and read) or not-click. How do we(called agent) organize or rank the articles to maximize click-thru rates(CTR)? Is there a strategy(called policy in RL terminology) that leads to better relevance and hence improve CTR ? Using supervised learning for this problem would attempt to learn the P(click|article) independently for all articles. With RL, at each position we are trying to learn P(click|article1, article2, article3) considering the sequence of articles already seen by the user. We are maximizing for total future cumulative rewards(clicks) that we expect to recieve by recommending this article given the fact that the user has already seen article 1,2 and 3. Intuitively, it makes sense to consider the sequence of articles already seen because 100s of articles might be relevant to a user across various topics and if we end up showing articles from one topic we might risk user getting fatigue and quiting the page. Instead, RL tries to arrive at the optimal policy that balances diversity in topics that are of interest to the user maximizing potential future clicks from that user <br> Today, RL has been used in a couple of industries with varied success and hasnt yet become mainstream technique yet. RL has been applied to game playing with good success, one of the early wins being game of <a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" data-href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AlphaGo</a> in which a RL agent beats expert human player. Consumer Internet, Quantitative Trading, Resource management, Advertising and Manufacturing are other areas where RL has been successfully applied. One of the reason RL has been extensively applied to game playing is because of our ability to collect lots of data through simulation of the game. With real-world problems, the data collection is far more expensive and in some cases infeasible which is one of the reasons preventing wide-spread adoption of RL.</p><figure name="8503" id="8503" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*np20H_0sOxM_gqjQOR_8dQ.png" data-width="624" data-height="219" src="https://cdn-images-1.medium.com/max/800/1*np20H_0sOxM_gqjQOR_8dQ.png"><figcaption class="imageCaption">Modelling RL problem</figcaption></figure><p name="cb01" id="cb01" class="graf graf--p graf-after--figure">Next, we consider the mathematical representation of Reinforcement Learning problem called Markov Decision Process(MDP).MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward.An MDP involves following components</p><ul class="postList"><li name="e96e" id="e96e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Agent</strong> is learner and decision maker.</li><li name="5972" id="5972" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Environment</strong> comprises everything the agent interacts with.</li><li name="5711" id="5711" class="graf graf--li graf-after--li">Everytime agent interacts with environment by taking <strong class="markup--strong markup--li-strong">actions</strong>, the environment responds by changing its <strong class="markup--strong markup--li-strong">state</strong></li><li name="e971" id="e971" class="graf graf--li graf-after--li">For every action taken, the agent receives a scalar <strong class="markup--strong markup--li-strong">reward</strong> indicating goodness of action taken. Reward can be instantaneous or delayed.</li><li name="35ce" id="35ce" class="graf graf--li graf-after--li">What are the states and actions in my environment ?</li><li name="68bd" id="68bd" class="graf graf--li graf-after--li">Are my interactions with environment continuous and infinite or Is there a natural end state for interactions?</li><li name="e7e3" id="e7e3" class="graf graf--li graf-after--li">Are there infinitely many (large number) of states ?</li><li name="af99" id="af99" class="graf graf--li graf-after--li">Do we know the system model ? or Do we know how my environment behaves to actions in different states ?&gt;</li><li name="fea1" id="fea1" class="graf graf--li graf-after--li">Are the rewards delayed or instantaneous ? Does agent involve trial-and-error actions ? Is there a sequential decision making ?</li><li name="eb62" id="eb62" class="graf graf--li graf-after--li">How should I design my reward scheme ? what metric should I base my rewards upon ?</li><li name="e951" id="e951" class="graf graf--li graf-after--li">How far-sighted should your agent be?</li><li name="87fa" id="87fa" class="graf graf--li graf-after--li">Does the agent learn and act on the strategy for taking actions(policy) in environment simultaneously ?</li><li name="cb43" id="cb43" class="graf graf--li graf-after--li">Are your states complex to represent/featurize?</li></ul><p name="52a8" id="52a8" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Windy Grid World</strong> is a modification to the famous grid world problem. The problem concerns navigating a grid of 7 X 10 cells by taking a sequence of actions(UP,DOWN,LEFT,RIGHT) at each cell. The goal is to reach the end state/cell — (3,7).An action from a state that leads to (3,7) gets a reward of 0 and every other transition results in a reward of -1. We have additional constraint of wind: There is wind blowing upwards in the columns 4–9 and the wind shift the agent by +1 or +2 steps depending on column the agent moved. Find the optimal path (least cost) path possible to reach (3,7) from (3,0). <br> We will use python <a href="https://gym.openai.com/" data-href="https://gym.openai.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">openAI gym</a> for simulating the windygrid environment. Gym is a RL toolkit by open AI which provides hundreds of environments for researchers and developers to develop novel RL algorithms. Each environment is a definition of dynamics of a system which can be used for simulation with option for visualizing the environment. Enviroment provides ways to record rewards from taking actions in a state. In our case, resetting the environment takes the states to (3,0), our start state. The target state is (3,7). We have 70 possible states the agent can be in at anytime. Each state is represented as (row,column) index and stored as 2D arrays. In every state, there are 4 possible actions (0-UP, 1-RIGHT, 2-DOWN, 3-LEFT)</p><p name="77be" id="77be" class="graf graf--p graf-after--p">Most of the current literature of RL focus on different problems to demonstrate the various RL techniques which makes comparison harder. To address, we will use the windy grid world to demostrate DP, MC and TD based algorithms. We will learn the Q function and V function along with optimal policy pi*</p><p name="0212" id="0212" class="graf graf--p graf-after--p">Most of the current literature of RL focus on different problems to demonstrate the various RL techniques which makes comparison harder. To address, we will use the windy grid world to demostrate DP, MC and TD based algorithms. We will learn the Q function and V function along with optimal policy pi*</p><pre name="a712" id="a712" class="graf graf--pre graf-after--p">%<strong class="markup--strong markup--pre-strong">run</strong> helper.py<br><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">gym</strong><br><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">gym_gridworlds</strong><br>env = gym.make(&#39;WindyGridworld-v0&#39;)<br>env.reset()</pre><p name="2d4b" id="2d4b" class="graf graf--p graf-after--pre">Out[59]:</p><pre name="1bd8" id="1bd8" class="graf graf--pre graf-after--p">(3, 0)</pre><h3 name="683b" id="683b" class="graf graf--h3 graf-after--pre">Dynamic Programming</h3><p name="5d98" id="5d98" class="graf graf--p graf-after--h3">DP is a model-based RL technique which assumes the complete knowledge of the system is readily available. i.e transition probabilities and Rewards/incentive scheme. Below code, we try to solve the Bellman Equation iteratively using Generailzed Policy Iteration. We start with random policy and evaluate wrt. to it. Based on value function learnt as part of evaluation, we will update the policy to being greedy wrt. value function. Next, we again evaluate the new policy to get an updated value function and update the policy to be greedy wrt current value function and this proceeds recursively until policy doesnt change(we call this convergence criteria). At this time, we have optimal policy and hence can evalaute once to get optimal value function and optimal action-value function.</p><pre name="a93f" id="a93f" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">numpy</strong> <strong class="markup--strong markup--pre-strong">as</strong> <strong class="markup--strong markup--pre-strong">np</strong>  <br>    <em class="markup--em markup--pre-em">#initialize</em><br>    policy = np.ones([70, 4]) / 4<br>    V = np.zeros(70)<br>    Q = np.zeros([70,4])<br>        <br>    <em class="markup--em markup--pre-em">#step1 : policy evaluation(complete)</em><br>    overall = 0 <br>    <strong class="markup--strong markup--pre-strong">while</strong> True :<br>        cnt = 0<br>        <strong class="markup--strong markup--pre-strong">while</strong> True:<br>            gamma = 1<br>            cnt = cnt + 1<br>            <em class="markup--em markup--pre-em">#theta = 1e-8</em><br>            theta = 0.01<br>            delta = 0<br>            oldpolicy = policy<br>            Vtmp = np.zeros(70)<br>            <strong class="markup--strong markup--pre-strong">for</strong> s <strong class="markup--strong markup--pre-strong">in</strong> range(70):<br>                <em class="markup--em markup--pre-em">## store old value of V[s] to calculate delta</em><br>                v = V[s]<br>                v_new = 0<br>                <strong class="markup--strong markup--pre-strong">for</strong> a <strong class="markup--strong markup--pre-strong">in</strong> range(4):<br>                    env.S = (s/10,s%<strong class="markup--strong markup--pre-strong">10</strong>)<br>                    <em class="markup--em markup--pre-em">## Iterate over possible results of taking action a</em><br>                    ns,r,done, info = env.step(a)<br>                    <strong class="markup--strong markup--pre-strong">if</strong> ns == (3,7):<br>                        r = 0<br>                    v_new += policy[s][a]*(r+gamma*V[10 * ns[0] + ns[1]])<br>                delta = max(delta, abs(v-v_new))<br>                Vtmp[s] = v_new<br>            <em class="markup--em markup--pre-em">#print delta</em><br>            V =  Vtmp<br>            <strong class="markup--strong markup--pre-strong">if</strong> delta &lt; theta <strong class="markup--strong markup--pre-strong">or</strong> cnt == 1:<br>                <strong class="markup--strong markup--pre-strong">break</strong><br>        <br>        <em class="markup--em markup--pre-em">#step2 :Derive Q from V</em><br>        <strong class="markup--strong markup--pre-strong">for</strong> s <strong class="markup--strong markup--pre-strong">in</strong> range(70):<br>            <strong class="markup--strong markup--pre-strong">for</strong> a <strong class="markup--strong markup--pre-strong">in</strong> range(4):<br>                q_a = 0<br>                env.S = (s/10,s%<strong class="markup--strong markup--pre-strong">10</strong>)<br>                ns,r,done, info = env.step(a)<br>                <strong class="markup--strong markup--pre-strong">if</strong> ns == (3,7):<br>                    r = 0<br>                q_a += (r + gamma*V[10 * ns[0] + ns[1]])<br>                Q[s][a] = q_a  <br>                <br>        <em class="markup--em markup--pre-em">#step 3: policy improvement</em><br>        policy = np.zeros([70, 4]) / 4   <br>        <strong class="markup--strong markup--pre-strong">for</strong> s <strong class="markup--strong markup--pre-strong">in</strong> range(70):<br>            Q_s = Q[s]<br>            argmax = np.argwhere(Q_s == np.amax(Q_s)).flatten().tolist()<br>            <em class="markup--em markup--pre-em">## Create Stochastic policy if multiple actions yield best results</em><br>            prob = 1.0/len(argmax)<br>            <strong class="markup--strong markup--pre-strong">for</strong> index <strong class="markup--strong markup--pre-strong">in</strong> argmax:<br>                policy[s][index] = prob<br>        <br>        <em class="markup--em markup--pre-em">#step 4: check if policy converged</em><br>        <strong class="markup--strong markup--pre-strong">if</strong> np.array_equal(oldpolicy,policy):<br>            <strong class="markup--strong markup--pre-strong">print</strong> &quot;converged after &quot;,overall,&quot; iterations&quot;<br>            <strong class="markup--strong markup--pre-strong">break</strong><br>        <strong class="markup--strong markup--pre-strong">else</strong>:<br>            None <em class="markup--em markup--pre-em">#print &quot;iterate&quot;</em><br>            overall = overall + 1<br>        oldpolicy = policy</pre><pre name="3b24" id="3b24" class="graf graf--pre graf-after--pre">converged after  127  iterations</pre><p name="63c5" id="63c5" class="graf graf--p graf-after--pre">The above code accomplishes the following steps (iteratively approaching optimality)<br><strong class="markup--strong markup--p-strong">step-0 </strong>: Initialization of policy to be equiprobable (ie. in each state the policy mentions taking any action with same prob.), value-function(V) is set to 0. action-value function(Q) is set to 0.<br><strong class="markup--strong markup--p-strong">step-1 </strong>: Policy Evaluation(PE) tries to arrive at as estimate of value function under the assumption of policy initialized.Vπ(s)=∑aπ(s,a)∑s′Pass′[Rass′+γVπ(s′)]<br><strong class="markup--strong markup--p-strong">step-2 </strong>: Get Q-function from V-function.Qπ(s,a)=Eπ{rt+γVπ(st+1)∣st=s,at=a}=∑s′Pass′[Rass′+γVπ(s′)]<br><strong class="markup--strong markup--p-strong">step-3 </strong>: Policy Improvement(PI) updates the policy greedily based on value-function(V and Q) estimated as part of step-1 and step-2.π′(s)=argmaxaQ(s,a)<br><strong class="markup--strong markup--p-strong">step-4 </strong>: Check for convergence — Does the policy (prev iter) == policy (current) ? If yes, stop the loop and call it a day ! converged. If not,go back to step and repeat until convergence.</p><figure name="689e" id="689e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LM4C2u_HyYUR2OSbeJuLGw.png" data-width="688" data-height="286" src="https://cdn-images-1.medium.com/max/800/1*LM4C2u_HyYUR2OSbeJuLGw.png"></figure><p name="f69c" id="f69c" class="graf graf--p graf-after--figure">Below code visualizes the optimal policy by showing optimal path to be followed from start to end(textually and visually). Total cost incurred is also shown. NOTE: The arrows might look weird, but do note that there is the effect of wind acting upwards which makes the agent shift more than one cells at a time.</p><pre name="c7ab" id="c7ab" class="graf graf--pre graf-after--p">visualize_path(Q,(5,5))</pre><pre name="2dd7" id="2dd7" class="graf graf--pre graf-after--pre graf--trailing">output:<br>(5, 5) =&gt; (4, 6) =&gt; (2, 7) =&gt; (0, 8) =&gt; (0, 9) =&gt; (1, 9) =&gt; (2, 9) =&gt; (3, 9) =&gt; (4, 9) =&gt; (4, 8) =&gt; (3, 7)<br>-10 10<br>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹  ⮞ ⮟ <br>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮟ <br>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮞  ⏹ ⮟ <br>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ✌  ⏹ ⮟ <br>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮞  ⏹ ⮜  ⮜ <br>⏹ ⏹ ⏹ ⏹ ⏹ ⮞  ⏹ ⏹ ⏹ ⏹ <br>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹</pre></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/an-illustrated-overview-of-reinforcement-learning-ccc47ae43b6">https://medium.com/@prdeepak.babu/an-illustrated-overview-of-reinforcement-learning-ccc47ae43b6</a></p>
