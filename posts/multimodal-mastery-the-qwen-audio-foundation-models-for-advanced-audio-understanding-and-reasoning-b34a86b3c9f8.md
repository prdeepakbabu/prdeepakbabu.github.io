---
title: "Multimodal Mastery: The Qwen Audio Foundation Models for Advanced Audio Understanding and Reasoning"
date: "2024-01-10"
coverImage: "https://cdn-images-1.medium.com/max/1024/1*GZ-3hMim6LtmAiRUGeWSXw.png"
canonical: "https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8"
mediumId: "b34a86b3c9f8"
source: "medium"
---

<section name="1234" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="6233" id="6233" class="graf graf--p graf-after--h3">As a follow-up to the <a href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" data-href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" class="markup--anchor markup--p-anchor" target="_blank">last blog on large multimodal audio models</a> (LMM), we’re here to explore an open-source large LMM audio model. While dozens of image and text MM models exist, those involving audio are scarce. Speech is a crucial modality that conveys complex signals beyond text, such as emotions, tones, and intentions in the human voice, and it carries valuable information about the speaker and environmental characteristics. In this post, we will delve into the OpenSource Audio FM series from Alibaba, known as ‘Qwen-Audio.’ The post is divided into three sections: (i) a brief overview of the Qwen-Audio paper from Alibaba, (ii) a code(<a href="https://medium.com/@prdeepak.babu/multimodal-models-qwen-audio-foundation-models-for-audio-understanding-reasoning-b34a86b3c9f8" data-href="https://medium.com/@prdeepak.babu/multimodal-models-qwen-audio-foundation-models-for-audio-understanding-reasoning-b34a86b3c9f8" class="markup--anchor markup--p-anchor" target="_blank">github</a>) to load a Qwen-Audio model from Hugging Face and demonstrate its capabilities, and (iii) the generation of a few audio samples under diverse acoustic conditions for exploration.</p><div name="9fe1" id="9fe1" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac" data-href="https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac"><strong class="markup--strong markup--mixtapeEmbed-strong">Audio Language Models and Multimodal Architecture</strong><br><em class="markup--em markup--mixtapeEmbed-em">Multimodal models are creating a synergy between previously separate research areas such as language, vision, and…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="7cce4d8743595a09cc1d74e7a3cbd732" data-thumbnail-img-id="0*YHKFAzTwEbZSr2j3" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*YHKFAzTwEbZSr2j3);"></a></div><h4 name="2e86" id="2e86" class="graf graf--h4 graf-after--mixtapeEmbed"><strong class="markup--strong markup--h4-strong">What is Qwen-Audio and Qwen-Audio-Chat ?</strong></h4><p name="655f" id="655f" class="graf graf--p graf-after--h4">Qwen-Audio (<a href="https://arxiv.org/pdf/2311.07919.pdf" data-href="https://arxiv.org/pdf/2311.07919.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">paper</a> and <a href="https://qwen-audio.github.io/Qwen-Audio/" data-href="https://qwen-audio.github.io/Qwen-Audio/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">code</a> | <a href="https://huggingface.co/Qwen/Qwen-Audio" data-href="https://huggingface.co/Qwen/Qwen-Audio" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">hugginface</a>) is a foundational audio model that is capable of handling diverse audio types(human speech, natural sound, music and songs) and audio tasks (ASR, acoustic scence classification,etc.). The Qwen-Audio model is trained on over 30+ diverse audio tasks like audio classification, speech recognition and emotion recognition). They demonstrate the Qwen-Audio model beats SoTA models on varied tasks indicating good performance in zero-shot setting. Author further demonstrate timestamp prediction shows improvement in grounding and grounding based QA tasks beyond speech signals, as well as ASR. Qwen-Audio is built using whisper-large-v2 as the audio encoder and using Qwen-7B decoder-only LLM model as the core component. The audio encoder is based on whisper-large-v2 composed of 640M parameters. Authors also modify the prompt tags to hierarchially organize tasks and datasets, to avoid loss in gains from interferance. The authors also train a Qwen-Audio-Chat model by fine-tuning Qwen-Audio model using supervised instruction fine-tuning on 30+ tasks and datasets. Chat model support multi-turn dialogs.</p><p name="2855" id="2855" class="graf graf--p graf-after--p">Key Contributions from the paper include</p><ul class="postList"><li name="07d0" id="07d0" class="graf graf--li graf-after--p">Introduces Qwen-Audio, a versatile multi-task audio-language model, alongside its extension Qwen-Audio-Chat for multi-turn dialogues, both of which are open-source to benefit the audio-text multimodal community.</li><li name="4d6f" id="4d6f" class="graf graf--li graf-after--li">Development a multi-task training framework to handle textual label variations across datasets, allowing for knowledge sharing and reducing interference, with Qwen-Audio excelling in over 30 tasks.</li><li name="4b66" id="4b66" class="graf graf--li graf-after--li">Demonstrates the importance of the SRWT task for audio-language pre-training, showing improvements in grounding tasks, question answering, and ASR performance.</li><li name="a627" id="a627" class="graf graf--li graf-after--li">Qwen-Audio outperforms similar models on benchmark tasks without task-specific fine-tuning, setting new records on Aishell1, cochlscene, ClothoAQA, and VocalSound datasets.</li></ul><figure name="cc7b" id="cc7b" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*3a_dzU1aliNkk7vmkL9oPw.png" data-width="744" data-height="833" src="https://cdn-images-1.medium.com/max/800/1*3a_dzU1aliNkk7vmkL9oPw.png"></figure><h4 name="13c3" id="13c3" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Loading and Experimenting Qwen-Audio</strong></h4><p name="9585" id="9585" class="graf graf--p graf-after--h4">We now write a few wrapper scripts to load Qwen-Audio-Chat model from huggingface, start a dialog and ask questions about the audio. It is to be noted, the Chat model supports multi-turn conversation or instruction i.e you can ask multiple questions about a single audio and retain context as you go along.</p><p name="b52d" id="b52d" class="graf graf--p graf-after--p">For the purpose of demo, we have created a few audio files in *.wav formation that includes diverse acoustic conditions and different speakers to test tasks from language id, speaker charecteristics to scence classification. The demo audio files can be found in the “data/” folder of <a href="https://github.com/prdeepakbabu/Qwen-AudioFM" data-href="https://github.com/prdeepakbabu/Qwen-AudioFM" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Qwen-AudioFM</a> repository.</p><div name="8300" id="8300" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/prdeepakbabu/Qwen-AudioFM/blob/4c83ca80746a98a7a288b76b0e2afb05a35a448a/main.py#L12C45-L15" data-href="https://github.com/prdeepakbabu/Qwen-AudioFM/blob/4c83ca80746a98a7a288b76b0e2afb05a35a448a/main.py#L12C45-L15" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/prdeepakbabu/Qwen-AudioFM/blob/4c83ca80746a98a7a288b76b0e2afb05a35a448a/main.py#L12C45-L15"><strong class="markup--strong markup--mixtapeEmbed-strong">Qwen-AudioFM/main.py at 4c83ca80746a98a7a288b76b0e2afb05a35a448a · prdeepakbabu/Qwen-AudioFM</strong><br><em class="markup--em markup--mixtapeEmbed-em">Experimenting capabilities of Audio FM series from Alibaba on 30+ audio tasks using textual prompts in the form of…</em>github.com</a><a href="https://github.com/prdeepakbabu/Qwen-AudioFM/blob/4c83ca80746a98a7a288b76b0e2afb05a35a448a/main.py#L12C45-L15" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="d45b353ac81df0080eb58aa751aed3b4" data-thumbnail-img-id="0*2qFcjRDp8q_HDVuZ" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*2qFcjRDp8q_HDVuZ);"></a></div><p name="bb84" id="bb84" class="graf graf--p graf-after--mixtapeEmbed">Execute the main.py where you will also configure the audio file and prompt or instruction. we start the dialog with “can you describe the scene”. The script loops and prompts for further question until it sees “end”. Once it encounters “end”, you’ll see a pretty-print of the conversation in the form user [USER] and [ASSISTANT] pairs.</p><div name="ec0a" id="ec0a" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/prdeepakbabu/Qwen-AudioFM" data-href="https://github.com/prdeepakbabu/Qwen-AudioFM" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/prdeepakbabu/Qwen-AudioFM"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - prdeepakbabu/Qwen-AudioFM: Experimenting capabilities of Audio FM series from Alibaba on…</strong><br><em class="markup--em markup--mixtapeEmbed-em">Experimenting capabilities of Audio FM series from Alibaba on 30+ audio tasks using textual prompts in the form of…</em>github.com</a><a href="https://github.com/prdeepakbabu/Qwen-AudioFM" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="06544776fe3e078b467dab734bb91722" data-thumbnail-img-id="0*RpSsnfFsuJ3h_OL8" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*RpSsnfFsuJ3h_OL8);"></a></div><h4 name="9077" id="9077" class="graf graf--h4 graf-after--mixtapeEmbed">Demo of capabilities</h4><p name="f847" id="f847" class="graf graf--p graf-after--h4">We will study a ~20s audio file with a male speech in english, having a background song being played from a tamil movie. The following output is a result of long conversation testing different audio and language capabilities.</p><p name="c853" id="c853" class="graf graf--p graf-after--p">First, lets check <strong class="markup--strong markup--p-strong">audio understanding and comprehension</strong> ability. we check diverse audio tasks like scence classification, sentiment detection, language identification, speaker age prediction, etc. The incorrect predictions are marked in italics. Overall, we see the audio reasoning is decent. It did detect there was a background song and predicted gender of speaker along with age correctly.</p><pre data-code-block-mode="0" spellcheck="false" name="47b7" id="47b7" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><strong class="markup--strong markup--pre-strong">USER</strong>: can you describe the scene in detail ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: A man is speaking in a calm and soothing voice, with a faint background music playing in the background.<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: what is his sentiment ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: Based on the voice, it sounds like this person is speaking in a calm and collected manner, so it&#39;s difficult to determine their sentiment with certainty. However, it sounds like they are willing to consider doing something differently and are open to suggestions.<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: what music is being played in background ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: The music being played in the background is a low quality recording of a song that consists of a repetitive beat, synth keys, and a faint male vocal singing in a foreign language. It sounds like a club or dance music.<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: which foreign language music ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: The song is in a foreign language, and <em class="markup--em markup--pre-em">it sounds like it might be Arabic</em>.<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: what is likely to be age of speaker ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: Based on the voice, it sounds like the speaker is in their 30s.<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: the background music seems to be an indian language. isnt it ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: Yes, the background music seems to be in an Indian language.<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: which indian language ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: The language in the <em class="markup--em markup--pre-em">background music is Hindi</em>.</span></pre><p name="a8a0" id="a8a0" class="graf graf--p graf-after--pre">Next, lets check for some core <strong class="markup--strong markup--p-strong">audio capabilites like ASR</strong>. can it recognize what the user said ? The Qwen-Audio-Chat model is accurate in recognizing the spoken words. It also emits written format complete with punctuations.</p><pre data-code-block-mode="0" spellcheck="false" name="a21b" id="a21b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><strong class="markup--strong markup--pre-strong">USER</strong>: what is the man saying ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: The man is speaking in English: &quot;I think we should just do this in a different way. <br>Yeah, there&#39;s not much of a waste. It&#39;s always been to be mixed up both. Okay, that&#39;s fine. <br>Yeah, thank you. But let&#39;s do it.&quot;.</span></pre><p name="d210" id="d210" class="graf graf--p graf-after--pre">Finally, lets see if the language(text) specific task performance remains intact. By this, I mean all things you would expect a LLM to be good act like factual QA and reasoning. One of the challenges in multimodal model development is alignment — cross-modal alignment. Audio is high bandwidth channel unlike text, so we want to understand if Audio modality training is not overpowering the text modality training.<br>Result — Interestingly, we dont see any major concerns with LLM-only tasks as well. It got the facts right and reasoning could have been better (got wrong answer to Vijay’s wife question) but I suspect we would get a better answer if we choose to ask the same to Qwen-LLM which is the backbone for the Qwen-Audio* models.</p><pre data-code-block-mode="0" spellcheck="false" name="2298" id="2298" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><strong class="markup--strong markup--pre-strong">USER</strong>: how would you respond to this person ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: I would respond by acknowledging their statement and expressing my willingness to consider doing things differently. For example, I could say something like: &quot;I understand what you&#39;re saying, and I&#39;m willing to explore different ways of doing things. Let&#39;s discuss this further and see if we can come up with a solution that works for everyone.&quot;<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: who is the president of united states ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: The president of the United States is Joe Biden. He was inaugurated on January 20, 2021.<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: Raju has two siblings Raghu and Shivu. Raju&#39;s dad is Vijay. How many kids does Vijay have ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: Vijay has three kids, Raju, Raghu, and Shivu.<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: How many kids does Vijay&#39;s wife have ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: <em class="markup--em markup--pre-em">Vijay&#39;s wife does not have any kids.</em><br><br><strong class="markup--strong markup--pre-strong">USER</strong>: what should we do if we detect fire during a hike ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: If you detect fire during a hike, the most important thing is to remain calm and assess the situation. Here are some steps you can take:<br><br>1. Stay calm and don&#39;t panic. Take a deep breath and assess the situation.<br>2. Determine the direction of the smoke and the location of the fire.<br>3. If the fire is small and contained, try to extinguish it using a fire extinguisher or a garden hose if available.<br>4. If the fire is out of control or spreading rapidly, leave the area immediately and alert the authorities.<br>5. If you are unable to leave the area, find a safe location and stay there until help arrives.<br>6. If you have a mobile phone, call the fire department or emergency services as soon as possible.<br>7. If you are in a wilderness area, try to find a designated trail or path to follow to escape the fire.<br>8. If you are in a forest, try to move away from the trees and towards open, grassy areas.<br>9. Avoid using matches, lighters, or other flammable materials while hiking.<br>10. Always carry a first aid kit, a map, and a compass with you while hiking.<br><br>Remember, prevention is the best way to avoid fire while hiking. Always follow Leave No Trace principles and be aware of your surroundings.<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: what does the person say in audio ?<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: The person says in English: &quot;I think we should just do this in a different way. Yeah, there&#39;s not much of a waste. It&#39;s always been to be mixed up both. Okay, that&#39;s fine. Yeah, thank you. But let&#39;s do it.&quot;<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: summarize the entire conversation we had<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: The conversation was about a man speaking in a calm and soothing voice, with a faint background music playing in the background. The man is speaking to someone about a situation and suggests doing things differently. He also mentions that there is not much waste and that the two things should be mixed up. The conversation ends with the man expressing gratitude and willingness to do something.<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: i meant not the audio but the chat here<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: Sure, here&#39;s a summary of our conversation:<br><br>* The conversation started with a question about the person&#39;s sentiment.<br>* The person responded by speaking in a calm and soothing voice, suggesting doing things differently.<br>* They mentioned that there is not much waste and that the two things should be mixed up.<br>* The conversation ended with the person expressing gratitude and willingness to do something.<br><br><strong class="markup--strong markup--pre-strong">USER</strong>: end<br><strong class="markup--strong markup--pre-strong">ASSISTANT</strong>: Alright, I&#39;m here if you have any more questions. Have a great day!</span></pre><p name="3d94" id="3d94" class="graf graf--p graf-after--pre">Overall, Qwen-Audio-Chat models looks to show decent performance among varied tasks for audio use-cases. The paper discusses quantitative results on varied benchmark datasets and tasks. While there a few popular works on audio FMs like speechGPT, AudioLM or AudioPaLM, most of these are not open-source. Qwen-Audio is one of the early open source Audio FM, which opens up new possibilites in powering voice-powered experiences.</p><p name="2cc6" id="2cc6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Related Posts</strong></p><div name="7688" id="7688" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" data-href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f"><strong class="markup--strong markup--mixtapeEmbed-strong">The Rise of Multimodal Large Speech &amp; Language Models</strong><br><em class="markup--em markup--mixtapeEmbed-em">In the age of foundational models that are based on deep learning architectures like transformer models, we can process…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/the-rise-of-multimodal-large-speech-language-models-4fc5ea34d04f" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="d72a4f6e2a6a50e7d5e804cb84e9740a" data-thumbnail-img-id="0*NnNDl_c-ZTERdkjl" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*NnNDl_c-ZTERdkjl);"></a></div><div name="a9a8" id="a9a8" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/@prdeepak.babu/harnessing-the-power-of-synthetic-data-in-the-era-of-large-language-models-llms-and-generative-ai-f67763256d44" data-href="https://medium.com/@prdeepak.babu/harnessing-the-power-of-synthetic-data-in-the-era-of-large-language-models-llms-and-generative-ai-f67763256d44" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@prdeepak.babu/harnessing-the-power-of-synthetic-data-in-the-era-of-large-language-models-llms-and-generative-ai-f67763256d44"><strong class="markup--strong markup--mixtapeEmbed-strong">Harnessing the Power of Synthetic Data in the Era of Large Language Models (LLMs) and Generative AI</strong><br><em class="markup--em markup--mixtapeEmbed-em">As we transitioned from the traditional AI methods of the 2010s to deep learning-based approaches in the 2020s, there’s…</em>medium.com</a><a href="https://medium.com/@prdeepak.babu/harnessing-the-power-of-synthetic-data-in-the-era-of-large-language-models-llms-and-generative-ai-f67763256d44" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="0536c6d9f4031cadd319f985904f133b" data-thumbnail-img-id="1*HJ4Dx9qv4f7oyvquhIHKZA.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*HJ4Dx9qv4f7oyvquhIHKZA.png);"></a></div><div name="c75e" id="c75e" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed graf--trailing"><a href="https://blog.gopenai.com/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0" data-href="https://blog.gopenai.com/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://blog.gopenai.com/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0"><strong class="markup--strong markup--mixtapeEmbed-strong">From N-grams to GPT-4: The Meteoric Rise of Large Language Models</strong><br><em class="markup--em markup--mixtapeEmbed-em">Large Language Models (LLMs) have caused a paradigm shift in the way NLP is traditionally done i.e anything to do with…</em>blog.gopenai.com</a><a href="https://blog.gopenai.com/from-n-grams-to-gpt-4-the-meteoric-rise-of-large-language-models-d9e064ec7bf0" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="ac46aa495334186ffed9a4e91f29c4b2" data-thumbnail-img-id="1*ChubSji0BNPU499IUSz-Rw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*ChubSji0BNPU499IUSz-Rw.png);"></a></div></div></div></section>

<hr>

<p><em>Originally published on Medium:</em> <a href="https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8">https://medium.com/@prdeepak.babu/multimodal-mastery-the-qwen-audio-foundation-models-for-advanced-audio-understanding-and-reasoning-b34a86b3c9f8</a></p>
